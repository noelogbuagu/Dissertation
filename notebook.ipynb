{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Property Prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant area codes\n",
    "area_codes = [\n",
    "    'E08000011', 'E08000012', 'E11000002', 'E08000014', 'E08000013',\n",
    "    'E08000007', 'E06000007', 'E08000010', 'E08000015'\n",
    "]\n",
    "\n",
    "# Regions in Merseyside\n",
    "regions = [\n",
    "    'Prenton', 'Newton-Le-Willows', 'Birkenhead',\n",
    "    'Wirral', 'Bootle', 'St Helens', 'Wallasey', 'Southport',\n",
    "    'Prescot', 'Wigan', 'Widnes', 'Neston', 'Warrington',\n",
    "    'Ellesmere Port', 'Wilmslow', 'Coniston', 'Stockport', 'Northwood',\n",
    "    'Crewe', 'Winsford', 'Merseyside', 'Sefton', 'Wirral', 'Liverpool', 'Knowsley'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_column(df, column_name, mapping, remove_values=None, new_dtype='category'):\n",
    "    \"\"\"\n",
    "    Standardise a DataFrame column by applying a mapping, optionally removing specific values, \n",
    "    and changing the data type.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column to standardize.\n",
    "    mapping (dict): A dictionary mapping old values to new values.\n",
    "    remove_values (list, optional): A list of values to remove from the column. Default is None.\n",
    "    new_dtype (str, optional): The new data type for the column. Default is 'category'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the standardized column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Optionally remove rows with specific values\n",
    "    if remove_values:\n",
    "        df = df[~df[column_name].isin(remove_values)]\n",
    "\n",
    "    # Apply the mapping to replace old values with new values\n",
    "    df[column_name] = df[column_name].replace(mapping).astype(new_dtype)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, new_column_names):\n",
    "    \"\"\"\n",
    "    Rename the columns of a DataFrame using a provided list of new column names.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame whose columns need to be renamed.\n",
    "    new_column_names (list): A list of new column names.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    # Check if the number of new column names matches the number of columns in the DataFrame\n",
    "    if len(new_column_names) != df.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"The number of new column names must match the number of columns in the DataFrame.\")\n",
    "\n",
    "    # Rename the columns\n",
    "    df.columns = new_column_names\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices Paid Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_lookup_postcodes(postcodes, batch_size=100):\n",
    "    \"\"\"\n",
    "    Perform a bulk lookup for postcodes using the postcodes.io API.\n",
    "\n",
    "    Parameters:\n",
    "    postcodes (list): A list of postcodes to lookup.\n",
    "    batch_size (int): The number of postcodes to include in each batch (max 100).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping postcodes to their respective data, including latitude, longitude, and termination data if applicable.\n",
    "    \"\"\"\n",
    "    url = \"https://api.postcodes.io/postcodes\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    results = {}\n",
    "\n",
    "    # Process the postcodes in batches\n",
    "    for i in range(0, len(postcodes), batch_size):\n",
    "        batch = postcodes[i:i+batch_size]\n",
    "        data = {\"postcodes\": batch}\n",
    "\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            for result in response_data['result']:\n",
    "                postcode = result['query']\n",
    "                if result['result'] is not None:\n",
    "                    results[postcode] = {\n",
    "                        'longitude': result['result']['longitude'],\n",
    "                        'latitude': result['result']['latitude'],\n",
    "                        'is_terminated': False\n",
    "                    }\n",
    "                else:\n",
    "                    # If the postcode is terminated, set to None and mark as terminated\n",
    "                    results[postcode] = {\n",
    "                        'longitude': None,\n",
    "                        'latitude': None,\n",
    "                        'is_terminated': True\n",
    "                    }\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to retrieve data for batch starting at {i}: {response.status_code}\")\n",
    "        time.sleep(0.1)  # To avoid rate limiting\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_terminated_postcodes(postcode_data):\n",
    "    \"\"\"\n",
    "    Handle postcodes that are marked as terminated by checking the terminated postcode API.\n",
    "\n",
    "    Parameters:\n",
    "    postcode_data (dict): A dictionary of postcodes with their data, including termination status.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated dictionary with longitude and latitude for terminated postcodes if available.\n",
    "    \"\"\"\n",
    "    terminated_postcodes = [postcode for postcode,\n",
    "                            data in postcode_data.items() if data['is_terminated']]\n",
    "\n",
    "    if not terminated_postcodes:\n",
    "        return postcode_data  # No terminated postcodes to handle\n",
    "\n",
    "    # Query terminated postcodes in bulk (max 100 at a time)\n",
    "    url = \"https://api.postcodes.io/terminated_postcodes\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    batch_size = 100\n",
    "\n",
    "    for i in range(0, len(terminated_postcodes), batch_size):\n",
    "        batch = terminated_postcodes[i:i+batch_size]\n",
    "        data = {\"postcodes\": batch}\n",
    "\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            for result in response_data['result']:\n",
    "                postcode = result['query']\n",
    "                if result['result'] is not None:\n",
    "                    postcode_data[postcode]['longitude'] = result['result']['longitude']\n",
    "                    postcode_data[postcode]['latitude'] = result['result']['latitude']\n",
    "                else:\n",
    "                    # If still no data, keep as None\n",
    "                    print(f\"No data found for terminated postcode: {postcode}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to retrieve data for terminated postcodes batch starting at {i}: {response.status_code}\")\n",
    "        time.sleep(0.1)  # To avoid rate limiting\n",
    "\n",
    "    return postcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_long_lat_columns(df, postcode_column='postcode'):\n",
    "    \"\"\"\n",
    "    Add longitude and latitude columns to the DataFrame based on the postcode column using Bulk Lookup.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame with a postcode column.\n",
    "    postcode_column (str): The name of the column containing postcodes. Default is 'postcode'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with added 'longitude' and 'latitude' columns.\n",
    "    \"\"\"\n",
    "    postcodes = df[postcode_column].unique().tolist()\n",
    "    postcode_data = bulk_lookup_postcodes(postcodes)\n",
    "    postcode_data = handle_terminated_postcodes(postcode_data)\n",
    "\n",
    "    # Map the longitude and latitude back to the original DataFrame\n",
    "    df['longitude'] = df[postcode_column].map(\n",
    "        lambda x: postcode_data[x]['longitude'])\n",
    "    df['latitude'] = df[postcode_column].map(\n",
    "        lambda x: postcode_data[x]['latitude'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for prices paid data\n",
    "def wrangle_prices_paid(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # drop complete duplicates from house_data\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # add column names to df\n",
    "    column_names = [\n",
    "        'transaction_id', 'price', 'transfer_date',\n",
    "        'postcode', 'property_type', 'is_old_or_new',\n",
    "        'property_tenure', 'house_number_or_name', 'unit_number',\n",
    "        'street', 'locality', 'town', 'district', 'county',\n",
    "        'ppd_transaction_category', 'record_status_monthly_file_only'\n",
    "    ]\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # convert transfer date to datetime\n",
    "    df['transfer_date'] = pd.to_datetime(df['transfer_date'])\n",
    "    \n",
    "    # sort data by teansaction date\n",
    "    df = df.sort_values('transfer_date', ascending=True)\n",
    "    \n",
    "    # filter data for freehold transactions in merseyside from 2013 to 2023\n",
    "    df = df[(df['county'] == 'MERSEYSIDE') & (\n",
    "        df['transfer_date'].dt.year >= 2013) & (df['transfer_date'].dt.year <= 2023)]\n",
    "\n",
    "    # convert price to float type\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    \n",
    "    # remove outliers in price by values in the bottom and top 5% of properties\n",
    "    # low, high = df['price'].quantile([0.05, 0.95])\n",
    "    # mask_area = df['price'].between(low, high)\n",
    "    # df = df[mask_area]\n",
    "\n",
    "    # convert ppd_transaction_category to category type\n",
    "    df['ppd_transaction_category'] = df['ppd_transaction_category'].astype(\n",
    "        'category')\n",
    "\n",
    "    # define mappings for replacement\n",
    "    property_type_mapping = {'T': 'Terraced', 'D': 'Detached', 'F': 'Flats/Maisonettes',\n",
    "                             'S': 'Semi-Detached', 'O': 'Other'}\n",
    "    old_or_new_mapping = {'N': 'Old', 'Y': 'New'}\n",
    "    property_tenure_mapping = {'F': 'Freehold', 'L': 'Leasehold'}\n",
    "\n",
    "    # standardize 'property_type' column\n",
    "    df = standardise_column(df, 'property_type', property_type_mapping)\n",
    "    # standardize 'is_old_or_new' column\n",
    "    df = standardise_column(df, 'is_old_or_new', old_or_new_mapping)\n",
    "    # standardize 'property_tenure' column and remove rows with 'U' before standardising\n",
    "    df = standardise_column(df, 'property_tenure',\n",
    "                            property_tenure_mapping, remove_values=['U'])\n",
    "\n",
    "    # convert capital case columns to title case\n",
    "    df['town'] = df['town'].str.title()\n",
    "    df['district'] = df['district'].str.title()\n",
    "    df['county'] = df['county'].str.title()\n",
    "    \n",
    "    # exclude rows with null postcode values\n",
    "    df = df[~df['postcode'].isnull()]\n",
    "    \n",
    "    # created latitude and longitude columns\n",
    "    df = add_long_lat_columns(df)\n",
    "\n",
    "    # drop redundant columns\n",
    "    df.drop(columns=['house_number_or_name', 'unit_number', 'locality',\n",
    "                     'street', 'record_status_monthly_file_only', 'postcode'],\n",
    "            inplace=True\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle the prices paid data\n",
    "prices_paid_data_df = wrangle_prices_paid(\n",
    "    filepath=\"raw_data/prices_paid.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "prices_paid_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Employment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for employment data\n",
    "def wrangle_employment(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # add column names to df\n",
    "    column_names = ['area_code', 'area_name', 'year', 'employment_rate',\n",
    "                    'confidence_interval_lower', 'confidence_interval_upper']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # filter for employment rate from 2013 to 2023\n",
    "    df = df[(df['year'] >= 2013) & (df['year'] <= 2023)]\n",
    "\n",
    "    # relevant columns\n",
    "    relevant_columns = ['area_code', 'area_name', 'year', 'employment_rate']\n",
    "    # select relevant columns\n",
    "    df = df[relevant_columns]\n",
    "\n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['area_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('year')\n",
    "    \n",
    "    # Convert the year column to string, then to datetime\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['year'] = pd.to_datetime(\n",
    "        df['year'], format='%Y')\n",
    "    # to keep only the year part\n",
    "    df['year'] = df['year'].dt.year\n",
    "        \n",
    "    # Convert employment rate from percentatage to rates\n",
    "    df['employment_rate'] = df['employment_rate']/100\n",
    "    \n",
    "    # aggregate the data\n",
    "    df = df.groupby(['area_code', 'area_name', 'year']).agg({\n",
    "        'employment_rate': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle the employment data\n",
    "employment_data_df = wrangle_employment(filepath=\"raw_data/employment_data.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "employment_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UK HPI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for hpi data\n",
    "def wrangle_hpi(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # select relevant columns\n",
    "    relevant_columns = [\n",
    "        'Date', 'RegionName', 'AreaCode', 'AveragePrice', 'Index', '1m%Change', '12m%Change', 'SalesVolume'\n",
    "    ]\n",
    "    df = df[relevant_columns]\n",
    "    \n",
    "    # change column names\n",
    "    column_names = ['date', 'region_name', 'area_code', 'average_price',\n",
    "                    'index', '1m%_change', '12m%_change', 'sales_volume']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # change date to dattime instead of object\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Filter for the date range\n",
    "    df = df[(df['date'] >= '2013-01-01') & (\n",
    "        df['date'] <= '2023-12-31')]\n",
    "    \n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['region_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('date')\n",
    "    \n",
    "    # drop complete duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle the HPI data\n",
    "hpi_data_df = wrangle_hpi(filepath=\"raw_data/uk_hpi.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "hpi_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Income Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for hpi data\n",
    "def wrangle_income(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # change column names\n",
    "    column_names = ['area_code', 'area_name', 'year',\n",
    "                        'gross_median_weekly_pay', 'confidence_interval_lower', 'confidence_interval_upper']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "\n",
    "    # relevant columns\n",
    "    relevant_columns = ['area_code', 'area_name',\n",
    "                        'year', 'gross_median_weekly_pay']\n",
    "    # select relevant columns\n",
    "    df = df[relevant_columns]\n",
    "    \n",
    "    \n",
    "    # Convert the year column to string, then to datetime\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['year'] = pd.to_datetime(\n",
    "        df['year'], format='%Y')\n",
    "\n",
    "    # to keep only the year part\n",
    "    df['year'] = df['year'].dt.year\n",
    "\n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['area_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('year')\n",
    "    \n",
    "    # aggregate the data\n",
    "    df = df.groupby(['area_code', 'area_name', 'year']).agg({\n",
    "        'gross_median_weekly_pay': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle the income data\n",
    "income_data_df = wrangle_income(filepath= \"raw_data/income_data.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "income_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files_in_folder(folder_path, pattern='*.csv'):\n",
    "    \"\"\"\n",
    "    Merge all CSV files in a given folder into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the folder containing the CSV files.\n",
    "    pattern (str): The pattern to match CSV files in the folder. Default is '*.csv'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A merged DataFrame containing all the data from the CSV files.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Get a list of all CSV files in the folder that match the pattern\n",
    "    csv_files = glob.glob(os.path.join(folder_path, pattern))\n",
    "\n",
    "    # Iterate over the list of files and read each one into a DataFrame\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "merged_crime_data = merge_csv_files_in_folder('raw_data/merseyside_csv_files')\n",
    "\n",
    "# Display the first few rows to verify the merged data\n",
    "merged_crime_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_crime_data(df):\n",
    "    \"\"\"\n",
    "    Cleans and processes crime data by selecting relevant columns, \n",
    "    converting the 'Month' to datetime, and sorting by 'Month'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing raw crime data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A cleaned DataFrame containing only relevant columns.\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    df = df[['Month', 'Longitude', 'Latitude', 'Crime type']]\n",
    "\n",
    "    # Convert 'Month' to datetime\n",
    "    df['Month'] = pd.to_datetime(df['Month'], format='%Y-%m').dt.to_period('M')\n",
    "\n",
    "    # Sort the DataFrame by 'Month'\n",
    "    df = df.sort_values(by='Month', ascending=True)\n",
    "\n",
    "    # Optionally, rename the columns for clarity\n",
    "    df = df.rename(\n",
    "        columns={'Month': 'crime_date', 'Crime type': 'crime', 'Longitude': 'long', 'Latitude': 'lat'})\n",
    "\n",
    "    return df\n",
    "\n",
    "crime_data_df = wrangle_crime_data(merged_crime_data)\n",
    "\n",
    "# Display the first few rows to verify the cleaning process\n",
    "crime_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flood Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_flood_data(api_url=\"https://environment.data.gov.uk/flood-monitoring/id/floodAreas?&_limit=5000\"):\n",
    "    \"\"\"\n",
    "    Fetch flood area data from the Environment Agency API.\n",
    "\n",
    "    Parameters:\n",
    "    api_url (str): The API endpoint to fetch flood data. Default is set to the flood areas endpoint.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing flood area data.\n",
    "    \"\"\"\n",
    "    response = requests.get(url=api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Convert the response to JSON format and extract relevant data\n",
    "        flood_data = response.json()\n",
    "        flood_areas = flood_data.get('items', [])\n",
    "        return flood_areas\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_flood_data(flood_areas):\n",
    "    \"\"\"\n",
    "    Prepare and clean flood area data by extracting relevant fields and converting to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    flood_areas (list): A list of dictionaries containing flood area data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the prepared flood area data.\n",
    "    \"\"\"\n",
    "    flood_areas_list = []\n",
    "    for area in flood_areas:\n",
    "        flood_areas_list.append({\n",
    "            'county': area.get('county'),\n",
    "            'description': area.get('description'),\n",
    "            'eaAreaName': area.get('eaAreaName'),\n",
    "            'lat': area.get('lat'),\n",
    "            'long': area.get('long'),\n",
    "            'riverOrSea': area.get('riverOrSea'),\n",
    "            'polygon': area.get('polygon')\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(flood_areas_list)\n",
    "\n",
    "    # Rename columns to more descriptive names\n",
    "    column_names = ['county', 'text_description',\n",
    "                    'area_name', 'lat', 'lon', 'water_source', 'polygon']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    df = df[df['area_name'].str.contains(\n",
    "        'mersey', case=False, na=False)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_geojson(uri):\n",
    "    \"\"\"\n",
    "    Fetch and parse GeoJSON data from a polygon URI.\n",
    "\n",
    "    Parameters:\n",
    "    uri (str): The URI pointing to the GeoJSON polygon.\n",
    "\n",
    "    Returns:\n",
    "    geometry: A Shapely geometry object created from the GeoJSON data.\n",
    "    \"\"\"\n",
    "    response = requests.get(uri)\n",
    "    if response.status_code == 200:\n",
    "        geojson = response.json()\n",
    "        # Access the 'geometry' from the first feature\n",
    "        if 'features' in geojson and len(geojson['features']) > 0:\n",
    "            geometry = geojson['features'][0]['geometry']\n",
    "            return shape(geometry)  # Convert GeoJSON to Shapely geometry\n",
    "        else:\n",
    "            print(f\"No features found in GeoJSON data from {uri}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve GeoJSON data from {uri}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_geodataframe(flood_areas_df):\n",
    "    \"\"\"\n",
    "    Convert the DataFrame to a GeoDataFrame by applying the GeoJSON fetching function.\n",
    "\n",
    "    Parameters:\n",
    "    flood_areas_df (DataFrame): The input DataFrame containing flood area data.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: A GeoDataFrame with Shapely geometries for each flood area.\n",
    "    \"\"\"\n",
    "    # Apply the fetch_geojson function to the 'polygon' column\n",
    "    flood_areas_df['geometry'] = flood_areas_df['polygon'].apply(fetch_geojson)\n",
    "    \n",
    "    flood_areas_df.drop('polygon', inplace = True)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    flood_areas_gdf = gpd.GeoDataFrame(flood_areas_df, geometry='geometry')\n",
    "\n",
    "    return flood_areas_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_flood_data():\n",
    "    \"\"\"\n",
    "    Complete wrangling process for flood data, including fetching, cleaning, filtering, and conversion to GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: A cleaned and filtered GeoDataFrame ready for spatial analysis.\n",
    "    \"\"\"\n",
    "    # Fetch flood data from the API\n",
    "    flood_areas = fetch_flood_data()\n",
    "\n",
    "    # Prepare and clean the data\n",
    "    df = prepare_flood_data(flood_areas)\n",
    "\n",
    "    # Convert to a GeoDataFrame\n",
    "    flood_areas_gdf = convert_to_geodataframe(df)\n",
    "\n",
    "    return flood_areas_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle the flood data and prepare it for analysis\n",
    "flood_data_gdf = wrangle_flood_data()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "flood_data_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_dataframes(prices_paid_df, employment_df, hpi_df, income_df, crime_df, flood_gdf):\n",
    "#     \"\"\"\n",
    "#     Merge multiple dataframes and a geodataframe on relevant keys and spatial relationships.\n",
    "\n",
    "#     Parameters:\n",
    "#     prices_paid_df (DataFrame): Cleaned prices paid data.\n",
    "#     employment_df (DataFrame): Employment data.\n",
    "#     hpi_df (DataFrame): House price index (HPI) data.\n",
    "#     income_df (DataFrame): Income data.\n",
    "#     crime_df (DataFrame): Crime data.\n",
    "#     flood_gdf (GeoDataFrame): Flood risk area data.\n",
    "\n",
    "#     Returns:\n",
    "#     DataFrame: A merged dataframe combining all relevant data.\n",
    "#     \"\"\"\n",
    "#     # Merge employment data based on town and year\n",
    "#     prices_paid_df['year'] = prices_paid_df['transfer_date'].dt.year\n",
    "#     merged_df = pd.merge(prices_paid_df, employment_df, left_on=[\n",
    "#                         'town', 'year'], right_on=['area_name', 'year'], how='left')\n",
    "\n",
    "#     # Merge HPI data based on area_code and date (rounding date to nearest month)\n",
    "#     merged_df = pd.merge(merged_df, hpi_df, left_on=[\n",
    "#                          'district', 'transfer_date'], right_on=['region_name', 'date'], how='left')\n",
    "\n",
    "#     # Merge income data based on area_code and year\n",
    "#     merged_df = pd.merge(merged_df, income_df, left_on=[\n",
    "#                         'district', 'year'], right_on=['area_name', 'year'], how='left')\n",
    "\n",
    "#     # Merge crime data based on nearest location (latitude and longitude)\n",
    "#     # For this, we'll need to create spatial points from the latitude and longitude\n",
    "#     prices_paid_gdf = gpd.GeoDataFrame(merged_df, geometry=gpd.points_from_xy(\n",
    "#         merged_df.longitude, merged_df.latitude))\n",
    "#     crime_gdf = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(\n",
    "#         crime_df.longitude, crime_df.latitude))\n",
    "\n",
    "#     # Spatial join to find the nearest crime data points\n",
    "#     merged_gdf = gpd.sjoin_nearest(\n",
    "#         prices_paid_gdf, crime_gdf, how='left', distance_col=\"crime_distance\")\n",
    "\n",
    "#     # Merge flood risk area data based on spatial join\n",
    "#     merged_gdf = gpd.sjoin(merged_gdf, flood_gdf, how='left', op='intersects')\n",
    "\n",
    "#     # Convert the final GeoDataFrame back to a DataFrame if needed\n",
    "#     final_df = pd.DataFrame(merged_gdf.drop(columns='geometry'))\n",
    "\n",
    "#     return final_df\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# merged_data = merge_dataframes(\n",
    "#     prices_paid_df, employment_df, hpi_df, income_df, crime_df, flood_gdf)\n",
    "\n",
    "# # Display the first few rows of the merged dataframe\n",
    "# print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_copy = prices_paid_data_df.copy()\n",
    "employment_copy = employment_data_df.copy()\n",
    "income_copy = income_data_df.copy()\n",
    "flood_copy = flood_data_gdf.copy()\n",
    "hpi_copy = hpi_data_df.copy()\n",
    "crime_copy = crime_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge employment data based on area_code and year\n",
    "prices_copy['year'] = prices_copy['transfer_date'].dt.year\n",
    "# Merge using district instead of town\n",
    "merged_prices_employment_copy = pd.merge(\n",
    "    prices_copy,\n",
    "    employment_copy,\n",
    "    left_on=['district', 'year'],\n",
    "    right_on=['area_name', 'year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill any missing values in the merged dataframe\n",
    "merged_prices_employment_copy.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "merged_prices_employment_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round transfer_date to the nearest month to match HPI data\n",
    "merged_prices_employment_copy['transfer_date_month'] = merged_prices_employment_copy['transfer_date'].dt.to_period(\n",
    "    'M')\n",
    "\n",
    "# Convert HPI date to the same period format for merging\n",
    "hpi_copy['date_month'] = pd.to_datetime(hpi_copy['date']).dt.to_period('M')\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_prices_employment_hpi_copy = pd.merge(\n",
    "    merged_prices_employment_copy,\n",
    "    hpi_copy,\n",
    "    left_on=['district', 'transfer_date_month'],\n",
    "    right_on=['region_name', 'date_month'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill any missing values in the merged dataframe\n",
    "merged_prices_employment_hpi_copy.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Drop the temporary 'transfer_date_month' and 'date_month' columns if needed\n",
    "merged_prices_employment_hpi_copy.drop(\n",
    "    columns=['transfer_date_month', 'date_month'], inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "merged_prices_employment_hpi_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming merged_prices_employment_hpi_copy is the DataFrame after merging prices, employment, and HPI data\n",
    "\n",
    "# Merge the dataframes based on district (area_name) and year\n",
    "merged_prices_employment_hpi_income_copy = pd.merge(\n",
    "    merged_prices_employment_hpi_copy,\n",
    "    income_copy,\n",
    "    left_on=['district', 'year'],\n",
    "    right_on=['area_name', 'year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill any missing values in the merged dataframe\n",
    "merged_prices_employment_hpi_income_copy.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "merged_prices_employment_hpi_income_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_crime_data(main_df, crime_df):\n",
    "    \"\"\"\n",
    "    Merge the combined property and economic data with crime data using spatial and temporal joins.\n",
    "\n",
    "    Parameters:\n",
    "    main_df (DataFrame): The DataFrame containing merged property, employment, HPI, and income data.\n",
    "    crime_df (DataFrame): The cleaned crime data DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A merged DataFrame with crime data added.\n",
    "    \"\"\"\n",
    "    # Convert the main DataFrame to a GeoDataFrame\n",
    "    main_gdf = gpd.GeoDataFrame(\n",
    "        main_df,\n",
    "        geometry=gpd.points_from_xy(main_df.longitude, main_df.latitude)\n",
    "    )\n",
    "\n",
    "    # Convert the crime DataFrame to a GeoDataFrame\n",
    "    crime_gdf = gpd.GeoDataFrame(\n",
    "        crime_df,\n",
    "        geometry=gpd.points_from_xy(crime_df.long, crime_df.lat)\n",
    "    )\n",
    "\n",
    "    # Perform spatial join (nearest) and filter by date\n",
    "    merged_gdf = gpd.sjoin_nearest(\n",
    "        main_gdf, crime_gdf, how='left', distance_col='crime_distance')\n",
    "\n",
    "    # Filter for the same date (month)\n",
    "    merged_gdf = merged_gdf[merged_gdf['transfer_date'].dt.to_period(\n",
    "        'M') == merged_gdf['crime_date']]\n",
    "\n",
    "    # Convert back to a DataFrame if needed\n",
    "    final_df = pd.DataFrame(merged_gdf.drop(columns='geometry'))\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "merged_prices_employment_hpi_income_crime_copy = merge_with_crime_data(\n",
    "    merged_prices_employment_hpi_income_copy, crime_copy)\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "merged_prices_employment_hpi_income_crime_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_prices_employment_hpi_income_crime_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_flood_data(main_df, flood_gdf):\n",
    "    \"\"\"\n",
    "    Merge the combined property and economic data with flood data using a spatial join.\n",
    "\n",
    "    Parameters:\n",
    "    main_df (DataFrame): The DataFrame containing merged property, employment, HPI, income, and crime data.\n",
    "    flood_gdf (GeoDataFrame): The GeoDataFrame containing flood risk area data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A merged DataFrame with flood risk data added.\n",
    "    \"\"\"\n",
    "    # Convert the main DataFrame to a GeoDataFrame, setting the CRS to WGS84 (EPSG:4326)\n",
    "    main_gdf = gpd.GeoDataFrame(\n",
    "        main_df,\n",
    "        geometry=gpd.points_from_xy(main_df.longitude, main_df.latitude),\n",
    "        # Assuming WGS84 coordinate system, which is common for latitude and longitude\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Check and set CRS for the flood GeoDataFrame if not already set\n",
    "    if flood_gdf.crs is None:\n",
    "        flood_gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    # Ensure the flood GeoDataFrame has the same CRS as the main GeoDataFrame\n",
    "    flood_gdf = flood_gdf.to_crs(main_gdf.crs)\n",
    "\n",
    "    # Remove 'index_right' if it already exists in the main_gdf\n",
    "    if 'index_right' in main_gdf.columns:\n",
    "        main_gdf = main_gdf.drop(columns=['index_right'])\n",
    "\n",
    "    # Perform a spatial join to add flood risk data to the main DataFrame\n",
    "    merged_gdf = gpd.sjoin(main_gdf, flood_gdf,\n",
    "                           how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "    # Convert back to a regular DataFrame if needed\n",
    "    final_df = pd.DataFrame(merged_gdf.drop(columns='geometry'))\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "final_merged_data = merge_with_flood_data(\n",
    "    merged_prices_employment_hpi_income_crime_copy, flood_data_gdf)\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "final_merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
