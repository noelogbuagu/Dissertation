{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Property Prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant area codes\n",
    "area_codes = [\n",
    "    'E08000011', 'E08000012', 'E11000002', 'E08000014', 'E08000013',\n",
    "    'E08000007', 'E06000007', 'E08000010', 'E08000015'\n",
    "]\n",
    "\n",
    "# Regions in Merseyside\n",
    "regions = [\n",
    "    'Prenton', 'Newton-Le-Willows', 'Birkenhead',\n",
    "    'Wirral', 'Bootle', 'St Helens', 'Wallasey', 'Southport',\n",
    "    'Prescot', 'Wigan', 'Widnes', 'Neston', 'Warrington',\n",
    "    'Ellesmere Port', 'Wilmslow', 'Coniston', 'Stockport', 'Northwood',\n",
    "    'Crewe', 'Winsford', 'Merseyside', 'Sefton', 'Wirral', 'Liverpool', 'Knowsley'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_column(df, column_name, mapping, remove_values=None, new_dtype='category'):\n",
    "    \"\"\"\n",
    "    Standardise a DataFrame column by applying a mapping, optionally removing specific values, \n",
    "    and changing the data type.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column to standardize.\n",
    "    mapping (dict): A dictionary mapping old values to new values.\n",
    "    remove_values (list, optional): A list of values to remove from the column. Default is None.\n",
    "    new_dtype (str, optional): The new data type for the column. Default is 'category'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the standardized column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Optionally remove rows with specific values\n",
    "    if remove_values:\n",
    "        df = df[~df[column_name].isin(remove_values)]\n",
    "\n",
    "    # Apply the mapping to replace old values with new values\n",
    "    df[column_name] = df[column_name].replace(mapping).astype(new_dtype)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, new_column_names):\n",
    "    \"\"\"\n",
    "    Rename the columns of a DataFrame using a provided list of new column names.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame whose columns need to be renamed.\n",
    "    new_column_names (list): A list of new column names.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    # Check if the number of new column names matches the number of columns in the DataFrame\n",
    "    if len(new_column_names) != df.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"The number of new column names must match the number of columns in the DataFrame.\")\n",
    "\n",
    "    # Rename the columns\n",
    "    df.columns = new_column_names\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices Paid Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_lookup_postcodes(postcodes, batch_size=100):\n",
    "    \"\"\"\n",
    "    Perform a bulk lookup for postcodes using the postcodes.io API.\n",
    "\n",
    "    Parameters:\n",
    "    postcodes (list): A list of postcodes to lookup.\n",
    "    batch_size (int): The number of postcodes to include in each batch (max 100).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping postcodes to their respective data, including latitude, longitude, and termination data if applicable.\n",
    "    \"\"\"\n",
    "    url = \"https://api.postcodes.io/postcodes\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    results = {}\n",
    "\n",
    "    # Process the postcodes in batches\n",
    "    for i in range(0, len(postcodes), batch_size):\n",
    "        batch = postcodes[i:i+batch_size]\n",
    "        data = {\"postcodes\": batch}\n",
    "\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            for result in response_data['result']:\n",
    "                postcode = result['query']\n",
    "                if result['result'] is not None:\n",
    "                    results[postcode] = {\n",
    "                        'longitude': result['result']['longitude'],\n",
    "                        'latitude': result['result']['latitude'],\n",
    "                        'is_terminated': False\n",
    "                    }\n",
    "                else:\n",
    "                    # If the postcode is terminated, set to None and mark as terminated\n",
    "                    results[postcode] = {\n",
    "                        'longitude': None,\n",
    "                        'latitude': None,\n",
    "                        'is_terminated': True\n",
    "                    }\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to retrieve data for batch starting at {i}: {response.status_code}\")\n",
    "        time.sleep(0.1)  # To avoid rate limiting\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_terminated_postcodes(postcode_data):\n",
    "    \"\"\"\n",
    "    Handle postcodes that are marked as terminated by checking the terminated postcode API.\n",
    "\n",
    "    Parameters:\n",
    "    postcode_data (dict): A dictionary of postcodes with their data, including termination status.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated dictionary with longitude and latitude for terminated postcodes if available.\n",
    "    \"\"\"\n",
    "    terminated_postcodes = [postcode for postcode,\n",
    "                            data in postcode_data.items() if data['is_terminated']]\n",
    "\n",
    "    if not terminated_postcodes:\n",
    "        return postcode_data  # No terminated postcodes to handle\n",
    "\n",
    "    # Query terminated postcodes in bulk (max 100 at a time)\n",
    "    url = \"https://api.postcodes.io/terminated_postcodes\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    batch_size = 100\n",
    "\n",
    "    for i in range(0, len(terminated_postcodes), batch_size):\n",
    "        batch = terminated_postcodes[i:i+batch_size]\n",
    "        data = {\"postcodes\": batch}\n",
    "\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            for result in response_data['result']:\n",
    "                postcode = result['query']\n",
    "                if result['result'] is not None:\n",
    "                    postcode_data[postcode]['longitude'] = result['result']['longitude']\n",
    "                    postcode_data[postcode]['latitude'] = result['result']['latitude']\n",
    "                else:\n",
    "                    # print(f\"No data found for terminated postcode: {postcode}\")\n",
    "                    continue\n",
    "        else:\n",
    "            # print(\n",
    "            #     f\"Failed to retrieve data for terminated postcodes batch starting at {i}: {response.status_code}\")\n",
    "            continue\n",
    "        time.sleep(0.1)  # To avoid rate limiting\n",
    "\n",
    "    return postcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_long_lat_columns(df, postcode_column='postcode'):\n",
    "    \"\"\"\n",
    "    Add longitude and latitude columns to the DataFrame based on the postcode column using Bulk Lookup.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame with a postcode column.\n",
    "    postcode_column (str): The name of the column containing postcodes. Default is 'postcode'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with added 'longitude' and 'latitude' columns.\n",
    "    \"\"\"\n",
    "    postcodes = df[postcode_column].unique().tolist()\n",
    "    postcode_data = bulk_lookup_postcodes(postcodes)\n",
    "    postcode_data = handle_terminated_postcodes(postcode_data)\n",
    "\n",
    "    # Map the longitude and latitude back to the original DataFrame\n",
    "    df['longitude'] = df[postcode_column].map(\n",
    "        lambda x: postcode_data[x]['longitude'])\n",
    "    df['latitude'] = df[postcode_column].map(\n",
    "        lambda x: postcode_data[x]['latitude'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for prices paid data\n",
    "def wrangle_prices_paid(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # drop complete duplicates from house_data\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # add column names to df\n",
    "    column_names = [\n",
    "        'transaction_id', 'price', 'transfer_date',\n",
    "        'postcode', 'property_type', 'is_old_or_new',\n",
    "        'property_tenure', 'house_number_or_name', 'unit_number',\n",
    "        'street', 'locality', 'town', 'district', 'county',\n",
    "        'ppd_transaction_category', 'record_status_monthly_file_only'\n",
    "    ]\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # convert transfer date to datetime\n",
    "    df['transfer_date'] = pd.to_datetime(df['transfer_date'])\n",
    "    \n",
    "    # sort data by teansaction date\n",
    "    df = df.sort_values('transfer_date', ascending=True)\n",
    "    \n",
    "    # filter data for freehold transactions in merseyside from 2013 to 2023\n",
    "    df = df[(df['county'] == 'MERSEYSIDE') & (\n",
    "        df['transfer_date'].dt.year >= 2013) & (df['transfer_date'].dt.year <= 2023)]\n",
    "\n",
    "    # convert price to float type\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    \n",
    "    # remove outliers in price by values in the bottom and top 5% of properties\n",
    "    low, high = df['price'].quantile([0.05, 0.95])\n",
    "    mask_area = df['price'].between(low, high)\n",
    "    df = df[mask_area]\n",
    "\n",
    "    # convert ppd_transaction_category to category type\n",
    "    df['ppd_transaction_category'] = df['ppd_transaction_category'].astype(\n",
    "        'category')\n",
    "\n",
    "    # define mappings for replacement\n",
    "    property_type_mapping = {'T': 'Terraced', 'D': 'Detached', 'F': 'Flats/Maisonettes',\n",
    "                             'S': 'Semi-Detached', 'O': 'Other'}\n",
    "    old_or_new_mapping = {'N': 'Old', 'Y': 'New'}\n",
    "    property_tenure_mapping = {'F': 'Freehold', 'L': 'Leasehold'}\n",
    "\n",
    "    # standardize 'property_type' column\n",
    "    df = standardise_column(df, 'property_type', property_type_mapping)\n",
    "    # standardize 'is_old_or_new' column\n",
    "    df = standardise_column(df, 'is_old_or_new', old_or_new_mapping)\n",
    "    # standardize 'property_tenure' column and remove rows with 'U' before standardising\n",
    "    df = standardise_column(df, 'property_tenure',\n",
    "                            property_tenure_mapping, remove_values=['U'])\n",
    "\n",
    "    # convert capital case columns to title case\n",
    "    df['town'] = df['town'].str.title()\n",
    "    df['district'] = df['district'].str.title()\n",
    "    df['county'] = df['county'].str.title()\n",
    "    \n",
    "    # exclude rows with null postcode values\n",
    "    df = df[~df['postcode'].isnull()]\n",
    "    \n",
    "    # created latitude and longitude columns\n",
    "    df = add_long_lat_columns(df)\n",
    "\n",
    "    # drop redundant columns\n",
    "    df.drop(columns=['house_number_or_name', 'unit_number', 'locality',\n",
    "                     'street', 'record_status_monthly_file_only', 'postcode'],\n",
    "            inplace=True\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>price</th>\n",
       "      <th>transfer_date</th>\n",
       "      <th>property_type</th>\n",
       "      <th>is_old_or_new</th>\n",
       "      <th>property_tenure</th>\n",
       "      <th>town</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>ppd_transaction_category</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18351985</th>\n",
       "      <td>{EF89E3A8-2BD1-4347-8B9B-F2CEEB2E62DC}</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Flats/Maisonettes</td>\n",
       "      <td>Old</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>Prenton</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.038801</td>\n",
       "      <td>53.384498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18251630</th>\n",
       "      <td>{CD1FD346-02E2-40B9-AD20-AF02A78999D1}</td>\n",
       "      <td>113000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Semi-Detached</td>\n",
       "      <td>Old</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Sefton</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-2.946158</td>\n",
       "      <td>53.519373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18458777</th>\n",
       "      <td>{3008D681-978E-4FB2-B152-5077F80C64CF}</td>\n",
       "      <td>79500.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>Old</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>Birkenhead</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.019912</td>\n",
       "      <td>53.376715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18512520</th>\n",
       "      <td>{554C7E6D-FB60-4BF3-AEF0-F18802D4C110}</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Detached</td>\n",
       "      <td>Old</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>Newton-Le-Willows</td>\n",
       "      <td>St Helens</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-2.635595</td>\n",
       "      <td>53.479038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18275462</th>\n",
       "      <td>{3A27DE8C-0D42-41CC-8501-7367F7E98993}</td>\n",
       "      <td>115000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Semi-Detached</td>\n",
       "      <td>Old</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-2.904433</td>\n",
       "      <td>53.412073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  transaction_id     price transfer_date  \\\n",
       "18351985  {EF89E3A8-2BD1-4347-8B9B-F2CEEB2E62DC}   75000.0    2013-01-02   \n",
       "18251630  {CD1FD346-02E2-40B9-AD20-AF02A78999D1}  113000.0    2013-01-02   \n",
       "18458777  {3008D681-978E-4FB2-B152-5077F80C64CF}   79500.0    2013-01-02   \n",
       "18512520  {554C7E6D-FB60-4BF3-AEF0-F18802D4C110}  385000.0    2013-01-02   \n",
       "18275462  {3A27DE8C-0D42-41CC-8501-7367F7E98993}  115000.0    2013-01-02   \n",
       "\n",
       "              property_type is_old_or_new property_tenure               town  \\\n",
       "18351985  Flats/Maisonettes           Old       Leasehold            Prenton   \n",
       "18251630      Semi-Detached           Old        Freehold          Liverpool   \n",
       "18458777           Terraced           Old        Freehold         Birkenhead   \n",
       "18512520           Detached           Old        Freehold  Newton-Le-Willows   \n",
       "18275462      Semi-Detached           Old        Freehold          Liverpool   \n",
       "\n",
       "           district      county ppd_transaction_category  longitude   latitude  \n",
       "18351985     Wirral  Merseyside                        A  -3.038801  53.384498  \n",
       "18251630     Sefton  Merseyside                        A  -2.946158  53.519373  \n",
       "18458777     Wirral  Merseyside                        A  -3.019912  53.376715  \n",
       "18512520  St Helens  Merseyside                        A  -2.635595  53.479038  \n",
       "18275462  Liverpool  Merseyside                        A  -2.904433  53.412073  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the prices paid data\n",
    "prices_paid_data_df = wrangle_prices_paid(\n",
    "    filepath=\"raw_data/prices_paid.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "prices_paid_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Employment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for employment data\n",
    "def wrangle_employment(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # add column names to df\n",
    "    column_names = ['area_code', 'area_name', 'year', 'employment_rate',\n",
    "                    'confidence_interval_lower', 'confidence_interval_upper']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # filter for employment rate from 2013 to 2023\n",
    "    df = df[(df['year'] >= 2013) & (df['year'] <= 2023)]\n",
    "\n",
    "    # relevant columns\n",
    "    relevant_columns = ['area_code', 'area_name', 'year', 'employment_rate']\n",
    "    # select relevant columns\n",
    "    df = df[relevant_columns]\n",
    "\n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['area_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('year')\n",
    "    \n",
    "    # Convert the year column to string, then to datetime\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['year'] = pd.to_datetime(\n",
    "        df['year'], format='%Y')\n",
    "    # to keep only the year part\n",
    "    df['year'] = df['year'].dt.year\n",
    "        \n",
    "    # Convert employment rate from percentatage to rates\n",
    "    df['employment_rate'] = df['employment_rate']/100\n",
    "    \n",
    "    # aggregate the data\n",
    "    df = df.groupby(['area_code', 'area_name', 'year']).agg({\n",
    "        'employment_rate': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_code</th>\n",
       "      <th>area_name</th>\n",
       "      <th>year</th>\n",
       "      <th>employment_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   area_code   area_name  year  employment_rate\n",
       "0  E06000007  Warrington  2013            0.782\n",
       "1  E06000007  Warrington  2014            0.779\n",
       "2  E06000007  Warrington  2015            0.779\n",
       "3  E06000007  Warrington  2016            0.762\n",
       "4  E06000007  Warrington  2017            0.774"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the employment data\n",
    "employment_data_df = wrangle_employment(filepath=\"raw_data/employment_data.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "employment_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UK HPI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for hpi data\n",
    "def wrangle_hpi(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # select relevant columns\n",
    "    relevant_columns = [\n",
    "        'Date', 'RegionName', 'AreaCode', 'AveragePrice', 'Index', '1m%Change', '12m%Change', 'SalesVolume'\n",
    "    ]\n",
    "    df = df[relevant_columns]\n",
    "    \n",
    "    # change column names\n",
    "    column_names = ['date', 'region_name', 'area_code', 'average_price',\n",
    "                    'index', '1m%_change', '12m%_change', 'sales_volume']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # change date to dattime instead of object\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Filter for the date range\n",
    "    df = df[(df['date'] >= '2013-01-01') & (\n",
    "        df['date'] <= '2023-12-31')]\n",
    "    \n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['region_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('date')\n",
    "    \n",
    "    # drop complete duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>region_name</th>\n",
       "      <th>area_code</th>\n",
       "      <th>average_price</th>\n",
       "      <th>index</th>\n",
       "      <th>1m%_change</th>\n",
       "      <th>12m%_change</th>\n",
       "      <th>sales_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86092</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Knowsley</td>\n",
       "      <td>E08000011</td>\n",
       "      <td>105390.6729</td>\n",
       "      <td>97.958916</td>\n",
       "      <td>0.304925</td>\n",
       "      <td>-1.857656</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86105</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>E08000012</td>\n",
       "      <td>105140.5433</td>\n",
       "      <td>93.986552</td>\n",
       "      <td>-0.314031</td>\n",
       "      <td>-2.003400</td>\n",
       "      <td>292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86115</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>E11000002</td>\n",
       "      <td>118307.7900</td>\n",
       "      <td>95.814439</td>\n",
       "      <td>-0.183908</td>\n",
       "      <td>-1.070033</td>\n",
       "      <td>891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86196</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Sefton</td>\n",
       "      <td>E08000014</td>\n",
       "      <td>135940.7468</td>\n",
       "      <td>98.603056</td>\n",
       "      <td>0.431996</td>\n",
       "      <td>-0.460236</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86225</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>St Helens</td>\n",
       "      <td>E08000013</td>\n",
       "      <td>107387.2971</td>\n",
       "      <td>96.951534</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.660812</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date region_name  area_code  average_price      index  1m%_change  \\\n",
       "86092 2013-01-01    Knowsley  E08000011    105390.6729  97.958916    0.304925   \n",
       "86105 2013-01-01   Liverpool  E08000012    105140.5433  93.986552   -0.314031   \n",
       "86115 2013-01-01  Merseyside  E11000002    118307.7900  95.814439   -0.183908   \n",
       "86196 2013-01-01      Sefton  E08000014    135940.7468  98.603056    0.431996   \n",
       "86225 2013-01-01   St Helens  E08000013    107387.2971  96.951534    0.002608   \n",
       "\n",
       "       12m%_change  sales_volume  \n",
       "86092    -1.857656          67.0  \n",
       "86105    -2.003400         292.0  \n",
       "86115    -1.070033         891.0  \n",
       "86196    -0.460236         176.0  \n",
       "86225     0.660812         103.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the HPI data\n",
    "hpi_data_df = wrangle_hpi(filepath=\"raw_data/uk_hpi.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "hpi_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Income Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for hpi data\n",
    "def wrangle_income(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # change column names\n",
    "    column_names = ['area_code', 'area_name', 'year',\n",
    "                        'gross_median_weekly_pay', 'confidence_interval_lower', 'confidence_interval_upper']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "\n",
    "    # relevant columns\n",
    "    relevant_columns = ['area_code', 'area_name',\n",
    "                        'year', 'gross_median_weekly_pay']\n",
    "    # select relevant columns\n",
    "    df = df[relevant_columns]\n",
    "    \n",
    "    \n",
    "    # Convert the year column to string, then to datetime\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['year'] = pd.to_datetime(\n",
    "        df['year'], format='%Y')\n",
    "\n",
    "    # to keep only the year part\n",
    "    df['year'] = df['year'].dt.year\n",
    "\n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['area_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('year')\n",
    "    \n",
    "    # aggregate the data\n",
    "    df = df.groupby(['area_code', 'area_name', 'year']).agg({\n",
    "        'gross_median_weekly_pay': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_code</th>\n",
       "      <th>area_name</th>\n",
       "      <th>year</th>\n",
       "      <th>gross_median_weekly_pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2008</td>\n",
       "      <td>410.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2009</td>\n",
       "      <td>424.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2010</td>\n",
       "      <td>428.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2011</td>\n",
       "      <td>402.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2012</td>\n",
       "      <td>411.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   area_code   area_name  year  gross_median_weekly_pay\n",
       "0  E06000007  Warrington  2008                    410.2\n",
       "1  E06000007  Warrington  2009                    424.2\n",
       "2  E06000007  Warrington  2010                    428.5\n",
       "3  E06000007  Warrington  2011                    402.7\n",
       "4  E06000007  Warrington  2012                    411.8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the income data\n",
    "income_data_df = wrangle_income(filepath= \"raw_data/income_data.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "income_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crime ID</th>\n",
       "      <th>Month</th>\n",
       "      <th>Reported by</th>\n",
       "      <th>Falls within</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>LSOA code</th>\n",
       "      <th>LSOA name</th>\n",
       "      <th>Crime type</th>\n",
       "      <th>Last outcome category</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02bc77633b1f2f795a1776f92a20304619bb73ee60cb36...</td>\n",
       "      <td>2019-11</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>-2.797801</td>\n",
       "      <td>53.355197</td>\n",
       "      <td>On or near Lower Road</td>\n",
       "      <td>E01012391</td>\n",
       "      <td>Halton 008B</td>\n",
       "      <td>Vehicle crime</td>\n",
       "      <td>Unable to prosecute suspect</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>-2.875002</td>\n",
       "      <td>53.486621</td>\n",
       "      <td>On or near Bigdale Drive</td>\n",
       "      <td>E01006448</td>\n",
       "      <td>Knowsley 001A</td>\n",
       "      <td>Anti-social behaviour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>-2.849802</td>\n",
       "      <td>53.490299</td>\n",
       "      <td>On or near Depot Road</td>\n",
       "      <td>E01006448</td>\n",
       "      <td>Knowsley 001A</td>\n",
       "      <td>Anti-social behaviour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>-2.872082</td>\n",
       "      <td>53.487541</td>\n",
       "      <td>On or near Quernmore Road</td>\n",
       "      <td>E01006448</td>\n",
       "      <td>Knowsley 001A</td>\n",
       "      <td>Anti-social behaviour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-11</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>Merseyside Police</td>\n",
       "      <td>-2.871609</td>\n",
       "      <td>53.489432</td>\n",
       "      <td>On or near Watts Close</td>\n",
       "      <td>E01006448</td>\n",
       "      <td>Knowsley 001A</td>\n",
       "      <td>Anti-social behaviour</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Crime ID    Month  \\\n",
       "0  02bc77633b1f2f795a1776f92a20304619bb73ee60cb36...  2019-11   \n",
       "1                                                NaN  2019-11   \n",
       "2                                                NaN  2019-11   \n",
       "3                                                NaN  2019-11   \n",
       "4                                                NaN  2019-11   \n",
       "\n",
       "         Reported by       Falls within  Longitude   Latitude  \\\n",
       "0  Merseyside Police  Merseyside Police  -2.797801  53.355197   \n",
       "1  Merseyside Police  Merseyside Police  -2.875002  53.486621   \n",
       "2  Merseyside Police  Merseyside Police  -2.849802  53.490299   \n",
       "3  Merseyside Police  Merseyside Police  -2.872082  53.487541   \n",
       "4  Merseyside Police  Merseyside Police  -2.871609  53.489432   \n",
       "\n",
       "                    Location  LSOA code      LSOA name             Crime type  \\\n",
       "0      On or near Lower Road  E01012391    Halton 008B          Vehicle crime   \n",
       "1   On or near Bigdale Drive  E01006448  Knowsley 001A  Anti-social behaviour   \n",
       "2      On or near Depot Road  E01006448  Knowsley 001A  Anti-social behaviour   \n",
       "3  On or near Quernmore Road  E01006448  Knowsley 001A  Anti-social behaviour   \n",
       "4     On or near Watts Close  E01006448  Knowsley 001A  Anti-social behaviour   \n",
       "\n",
       "         Last outcome category  Context  \n",
       "0  Unable to prosecute suspect      NaN  \n",
       "1                          NaN      NaN  \n",
       "2                          NaN      NaN  \n",
       "3                          NaN      NaN  \n",
       "4                          NaN      NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_csv_files_in_folder(folder_path, pattern='*.csv'):\n",
    "    \"\"\"\n",
    "    Merge all CSV files in a given folder into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the folder containing the CSV files.\n",
    "    pattern (str): The pattern to match CSV files in the folder. Default is '*.csv'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A merged DataFrame containing all the data from the CSV files.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Get a list of all CSV files in the folder that match the pattern\n",
    "    csv_files = glob.glob(os.path.join(folder_path, pattern))\n",
    "\n",
    "    # Iterate over the list of files and read each one into a DataFrame\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "merged_crime_data = merge_csv_files_in_folder('raw_data/merseyside_csv_files')\n",
    "\n",
    "# Display the first few rows to verify the merged data\n",
    "merged_crime_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crime_date</th>\n",
       "      <th>long</th>\n",
       "      <th>lat</th>\n",
       "      <th>crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1296137</th>\n",
       "      <td>2013-01</td>\n",
       "      <td>-2.665400</td>\n",
       "      <td>53.474255</td>\n",
       "      <td>Other crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297883</th>\n",
       "      <td>2013-01</td>\n",
       "      <td>-3.049855</td>\n",
       "      <td>53.410883</td>\n",
       "      <td>Anti-social behaviour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297882</th>\n",
       "      <td>2013-01</td>\n",
       "      <td>-3.037867</td>\n",
       "      <td>53.409747</td>\n",
       "      <td>Other crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297881</th>\n",
       "      <td>2013-01</td>\n",
       "      <td>-3.036802</td>\n",
       "      <td>53.410503</td>\n",
       "      <td>Violent crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297880</th>\n",
       "      <td>2013-01</td>\n",
       "      <td>-3.040265</td>\n",
       "      <td>53.409987</td>\n",
       "      <td>Violent crime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        crime_date      long        lat                  crime\n",
       "1296137    2013-01 -2.665400  53.474255            Other crime\n",
       "1297883    2013-01 -3.049855  53.410883  Anti-social behaviour\n",
       "1297882    2013-01 -3.037867  53.409747            Other crime\n",
       "1297881    2013-01 -3.036802  53.410503          Violent crime\n",
       "1297880    2013-01 -3.040265  53.409987          Violent crime"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def wrangle_crime_data(df):\n",
    "    \"\"\"\n",
    "    Cleans and processes crime data by selecting relevant columns, \n",
    "    converting the 'Month' to datetime, and sorting by 'Month'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing raw crime data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A cleaned DataFrame containing only relevant columns.\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    df = df[['Month', 'Longitude', 'Latitude', 'Crime type']]\n",
    "\n",
    "    # Convert 'Month' to datetime\n",
    "    df['Month'] = pd.to_datetime(df['Month'], format='%Y-%m').dt.to_period('M')\n",
    "\n",
    "    # Sort the DataFrame by 'Month'\n",
    "    df = df.sort_values(by='Month', ascending=True)\n",
    "\n",
    "    # Optionally, rename the columns for clarity\n",
    "    df = df.rename(\n",
    "        columns={'Month': 'crime_date', 'Crime type': 'crime', 'Longitude': 'long', 'Latitude': 'lat'})\n",
    "\n",
    "    return df\n",
    "\n",
    "crime_data_df = wrangle_crime_data(merged_crime_data)\n",
    "\n",
    "# Display the first few rows to verify the cleaning process\n",
    "crime_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flood Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_flood_data(api_url=\"https://environment.data.gov.uk/flood-monitoring/id/floodAreas?&_limit=5000\"):\n",
    "    \"\"\"\n",
    "    Fetch flood area data from the Environment Agency API.\n",
    "\n",
    "    Parameters:\n",
    "    api_url (str): The API endpoint to fetch flood data. Default is set to the flood areas endpoint.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing flood area data.\n",
    "    \"\"\"\n",
    "    response = requests.get(url=api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Convert the response to JSON format and extract relevant data\n",
    "        flood_data = response.json()\n",
    "        flood_areas = flood_data.get('items', [])\n",
    "        return flood_areas\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_flood_data(flood_areas):\n",
    "    \"\"\"\n",
    "    Prepare and clean flood area data by extracting relevant fields and converting to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    flood_areas (list): A list of dictionaries containing flood area data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the prepared flood area data.\n",
    "    \"\"\"\n",
    "    flood_areas_list = []\n",
    "    for area in flood_areas:\n",
    "        flood_areas_list.append({\n",
    "            'county': area.get('county'),\n",
    "            'description': area.get('description'),\n",
    "            'eaAreaName': area.get('eaAreaName'),\n",
    "            'lat': area.get('lat'),\n",
    "            'long': area.get('long'),\n",
    "            'riverOrSea': area.get('riverOrSea'),\n",
    "            'polygon': area.get('polygon')\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(flood_areas_list)\n",
    "\n",
    "    # Rename columns to more descriptive names\n",
    "    column_names = ['county', 'text_description',\n",
    "                    'area_name', 'lat', 'lon', 'water_source', 'polygon']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    df = df[df['area_name'].str.contains(\n",
    "        'mersey', case=False, na=False)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_geojson(uri):\n",
    "    \"\"\"\n",
    "    Fetch and parse GeoJSON data from a polygon URI.\n",
    "\n",
    "    Parameters:\n",
    "    uri (str): The URI pointing to the GeoJSON polygon.\n",
    "\n",
    "    Returns:\n",
    "    geometry: A Shapely geometry object created from the GeoJSON data.\n",
    "    \"\"\"\n",
    "    response = requests.get(uri)\n",
    "    if response.status_code == 200:\n",
    "        geojson = response.json()\n",
    "        # Access the 'geometry' from the first feature\n",
    "        if 'features' in geojson and len(geojson['features']) > 0:\n",
    "            geometry = geojson['features'][0]['geometry']\n",
    "            return shape(geometry)  # Convert GeoJSON to Shapely geometry\n",
    "        else:\n",
    "            print(f\"No features found in GeoJSON data from {uri}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve GeoJSON data from {uri}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_geodataframe(flood_areas_df):\n",
    "    \"\"\"\n",
    "    Convert the DataFrame to a GeoDataFrame by applying the GeoJSON fetching function.\n",
    "\n",
    "    Parameters:\n",
    "    flood_areas_df (DataFrame): The input DataFrame containing flood area data.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: A GeoDataFrame with Shapely geometries for each flood area.\n",
    "    \"\"\"\n",
    "    # Apply the fetch_geojson function to the 'polygon' column\n",
    "    flood_areas_df['geometry'] = flood_areas_df['polygon'].apply(fetch_geojson)\n",
    "    \n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    flood_areas_gdf = gpd.GeoDataFrame(flood_areas_df, geometry='geometry')\n",
    "\n",
    "    return flood_areas_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_flood_data():\n",
    "    \"\"\"\n",
    "    Complete wrangling process for flood data, including fetching, cleaning, filtering, and conversion to GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: A cleaned and filtered GeoDataFrame ready for spatial analysis.\n",
    "    \"\"\"\n",
    "    # Fetch flood data from the API\n",
    "    flood_areas = fetch_flood_data()\n",
    "\n",
    "    # Prepare and clean the data\n",
    "    df = prepare_flood_data(flood_areas)\n",
    "\n",
    "    # Convert to a GeoDataFrame\n",
    "    flood_areas_gdf = convert_to_geodataframe(df)\n",
    "\n",
    "    return flood_areas_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>text_description</th>\n",
       "      <th>area_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>water_source</th>\n",
       "      <th>polygon</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Manchester</td>\n",
       "      <td>Land adjacent to the River Mersey at West Dids...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.41100</td>\n",
       "      <td>-2.24160</td>\n",
       "      <td>River Mersey</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>POLYGON ((-2.24472 53.4153, -2.24508 53.41532,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Manchester</td>\n",
       "      <td>Areas at risk include land and properties arou...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.51869</td>\n",
       "      <td>-2.22357</td>\n",
       "      <td>River Irk</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>MULTIPOLYGON (((-2.22293 53.51522, -2.22231 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Manchester, Stockport</td>\n",
       "      <td>Areas in the locality of Mauldeth Road, includ...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.43947</td>\n",
       "      <td>-2.23322</td>\n",
       "      <td>Cringle Brook</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>MULTIPOLYGON (((-2.22034 53.43457, -2.22038 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Rochdale</td>\n",
       "      <td>Areas at risk include properties on Park Road,...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.62409</td>\n",
       "      <td>-2.14482</td>\n",
       "      <td>Hey Brook</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>MULTIPOLYGON (((-2.14278 53.63016, -2.14274 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Derbyshire, Tameside</td>\n",
       "      <td>properties on the Etherow Industrial Estate, W...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.46075</td>\n",
       "      <td>-1.98511</td>\n",
       "      <td>River Etherow</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>POLYGON ((-1.98882 53.45935, -1.98882 53.45908...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   county                                   text_description  \\\n",
       "12             Manchester  Land adjacent to the River Mersey at West Dids...   \n",
       "22             Manchester  Areas at risk include land and properties arou...   \n",
       "27  Manchester, Stockport  Areas in the locality of Mauldeth Road, includ...   \n",
       "38               Rochdale  Areas at risk include properties on Park Road,...   \n",
       "54   Derbyshire, Tameside  properties on the Etherow Industrial Estate, W...   \n",
       "\n",
       "                    area_name       lat      lon   water_source  \\\n",
       "12  Gtr Mancs Mersey and Ches  53.41100 -2.24160   River Mersey   \n",
       "22  Gtr Mancs Mersey and Ches  53.51869 -2.22357      River Irk   \n",
       "27  Gtr Mancs Mersey and Ches  53.43947 -2.23322  Cringle Brook   \n",
       "38  Gtr Mancs Mersey and Ches  53.62409 -2.14482      Hey Brook   \n",
       "54  Gtr Mancs Mersey and Ches  53.46075 -1.98511  River Etherow   \n",
       "\n",
       "                                              polygon  \\\n",
       "12  http://environment.data.gov.uk/flood-monitorin...   \n",
       "22  http://environment.data.gov.uk/flood-monitorin...   \n",
       "27  http://environment.data.gov.uk/flood-monitorin...   \n",
       "38  http://environment.data.gov.uk/flood-monitorin...   \n",
       "54  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                                             geometry  \n",
       "12  POLYGON ((-2.24472 53.4153, -2.24508 53.41532,...  \n",
       "22  MULTIPOLYGON (((-2.22293 53.51522, -2.22231 53...  \n",
       "27  MULTIPOLYGON (((-2.22034 53.43457, -2.22038 53...  \n",
       "38  MULTIPOLYGON (((-2.14278 53.63016, -2.14274 53...  \n",
       "54  POLYGON ((-1.98882 53.45935, -1.98882 53.45908...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the flood data and prepare it for analysis\n",
    "flood_data_gdf = wrangle_flood_data()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "flood_data_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Price and Employment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_price_and_employment(prices_df, employment_df):\n",
    "    \"\"\"\n",
    "    Merge prices paid data with employment data.\n",
    "\n",
    "    Parameters:\n",
    "    prices_df (DataFrame): DataFrame containing prices paid data.\n",
    "    employment_df (DataFrame): DataFrame containing employment data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Merged DataFrame with employment data added.\n",
    "    \"\"\"\n",
    "    # Add year column to prices_df\n",
    "    prices_df['year'] = prices_df['transfer_date'].dt.year\n",
    "\n",
    "    # Merge the dataframes\n",
    "    merged_df = pd.merge(\n",
    "        prices_df,\n",
    "        employment_df,\n",
    "        left_on=['district', 'year'],\n",
    "        right_on=['area_name', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Forward fill any missing values\n",
    "    merged_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "merged_prices_employment = merge_price_and_employment(\n",
    "    prices_paid_data_df, employment_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with HPI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_hpi(prices_employment_df, hpi_df):\n",
    "    \"\"\"\n",
    "    Merge the combined prices and employment data with HPI data.\n",
    "\n",
    "    Parameters:\n",
    "    prices_employment_df (DataFrame): DataFrame after merging prices and employment data.\n",
    "    hpi_df (DataFrame): DataFrame containing HPI data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Merged DataFrame with HPI data added.\n",
    "    \"\"\"\n",
    "    # Round transfer_date to the nearest month to match HPI data\n",
    "    prices_employment_df['transfer_date_month'] = prices_employment_df['transfer_date'].dt.to_period(\n",
    "        'M')\n",
    "\n",
    "    # Convert HPI date to the same period format for merging\n",
    "    hpi_df['date_month'] = pd.to_datetime(hpi_df['date']).dt.to_period('M')\n",
    "\n",
    "    # Merge the dataframes\n",
    "    merged_df = pd.merge(\n",
    "        prices_employment_df,\n",
    "        hpi_df,\n",
    "        left_on=['district', 'transfer_date_month'],\n",
    "        right_on=['region_name', 'date_month'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Forward fill any missing values\n",
    "    merged_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    # Drop the temporary 'transfer_date_month' and 'date_month' columns if needed\n",
    "    merged_df.drop(columns=['transfer_date_month', 'date_month'], inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "merged_prices_employment_hpi = merge_with_hpi(\n",
    "    merged_prices_employment, hpi_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with Income Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_income(prices_employment_hpi_df, income_df):\n",
    "    \"\"\"\n",
    "    Merge the combined prices, employment, and HPI data with income data.\n",
    "\n",
    "    Parameters:\n",
    "    prices_employment_hpi_df (DataFrame): DataFrame after merging prices, employment, and HPI data.\n",
    "    income_df (DataFrame): DataFrame containing income data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Merged DataFrame with income data added.\n",
    "    \"\"\"\n",
    "    # Merge the dataframes based on district (area_name) and year\n",
    "    merged_df = pd.merge(\n",
    "        prices_employment_hpi_df,\n",
    "        income_df,\n",
    "        left_on=['district', 'year'],\n",
    "        right_on=['area_name', 'year'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Forward fill any missing values\n",
    "    merged_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "merged_prices_employment_hpi_income = merge_with_income(\n",
    "    merged_prices_employment_hpi, income_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_crime_density_columns(crime_df):\n",
    "    \"\"\"\n",
    "    Create a column for each unique crime type in the crime DataFrame \n",
    "    and calculate the density of each crime type at each location.\n",
    "    \n",
    "    Parameters:\n",
    "    crime_df (GeoDataFrame): The GeoDataFrame containing crime data with longitude and latitude.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing columns for the density of each crime type per location.\n",
    "    \"\"\"\n",
    "    # Round the latitude and longitude to group nearby crimes\n",
    "    crime_df['rounded_lat'] = crime_df['lat'].round(4)\n",
    "    crime_df['rounded_long'] = crime_df['long'].round(4)\n",
    "\n",
    "    # Get the unique crime types\n",
    "    crime_types = crime_df['crime'].unique()\n",
    "\n",
    "    # Initialize a dictionary to store crime densities\n",
    "    crime_density_dict = {}\n",
    "\n",
    "    # Calculate density for each crime type\n",
    "    for crime_type in crime_types:\n",
    "        crime_type_df = crime_df[crime_df['crime'] == crime_type]\n",
    "        crime_density = crime_type_df.groupby(['rounded_lat', 'rounded_long']).size(\n",
    "        ).reset_index(name=f'{crime_type}_density')\n",
    "        crime_density_dict[crime_type] = crime_density\n",
    "\n",
    "    # Merge all crime density DataFrames into one\n",
    "    crime_density_df = crime_density_dict[crime_types[0]]\n",
    "    for crime_type in crime_types[1:]:\n",
    "        crime_density_df = pd.merge(crime_density_df, crime_density_dict[crime_type],\n",
    "                                    on=['rounded_lat', 'rounded_long'], how='outer')\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    crime_density_df.fillna(0, inplace=True)\n",
    "\n",
    "    return crime_density_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_crime_density(main_df, crime_density_df):\n",
    "    \"\"\"\n",
    "    Merge the main DataFrame with the crime density data.\n",
    "    \n",
    "    Parameters:\n",
    "    main_df (DataFrame): The DataFrame containing merged property, employment, HPI, and income data.\n",
    "    crime_density_df (DataFrame): The DataFrame containing crime density information for each crime type.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A merged DataFrame with crime density data added.\n",
    "    \"\"\"\n",
    "    # Round the latitude and longitude in the main DataFrame to match the crime density DataFrame\n",
    "    main_df['rounded_lat'] = main_df['latitude'].round(4)\n",
    "    main_df['rounded_long'] = main_df['longitude'].round(4)\n",
    "\n",
    "    # Merge the crime density with the main DataFrame\n",
    "    merged_df = pd.merge(main_df, crime_density_df,\n",
    "                         left_on=['rounded_lat', 'rounded_long'],\n",
    "                         right_on=['rounded_lat', 'rounded_long'],\n",
    "                         how='left')\n",
    "\n",
    "    # Fill any missing densities with 0\n",
    "    for column in crime_density_df.columns:\n",
    "        if '_density' in column:\n",
    "            merged_df[column].fillna(0, inplace=True)\n",
    "\n",
    "    # Drop the temporary rounded columns if no longer needed\n",
    "    merged_df.drop(columns=['rounded_lat', 'rounded_long'], inplace=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the crime density DataFrame with separate columns for each crime type\n",
    "crime_density_df = create_crime_density_columns(crime_data_df)\n",
    "\n",
    "# 2. Merge the crime density data with the main DataFrame\n",
    "final_merged_data_with_crime_density = merge_with_crime_density(\n",
    "    merged_prices_employment_hpi_income, crime_density_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with Flood Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_geometries(gdf):\n",
    "    \"\"\"\n",
    "    Clean the geometries in a GeoDataFrame by buffering.\n",
    "    \n",
    "    Parameters:\n",
    "    gdf (GeoDataFrame): The GeoDataFrame with potentially invalid geometries.\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: The cleaned GeoDataFrame.\n",
    "    \"\"\"\n",
    "    gdf['geometry'] = gdf['geometry'].buffer(0)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_flood_proximity(main_df, flood_gdf):\n",
    "    \"\"\"\n",
    "    Categorize flood proximity for each property into five categories:\n",
    "    1. 'In Flood Area' (1): Within the flood area polygon.\n",
    "    2. 'High Risk' (2): Very close to the flood area (within 0.005 degrees).\n",
    "    3. 'Moderate Risk' (3): Near the flood area (within 0.01 degrees).\n",
    "    4. 'Low Risk' (4): Farther from the flood area (within 0.02 degrees).\n",
    "    5. 'No Risk' (5): Not close to the flood area.\n",
    "    \n",
    "    Parameters:\n",
    "    main_df (DataFrame): The main DataFrame containing property data.\n",
    "    flood_gdf (GeoDataFrame): The GeoDataFrame containing flood area data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The main DataFrame with a 'flood_proximity' column.\n",
    "    \"\"\"\n",
    "    # Convert the main DataFrame to a GeoDataFrame, setting the CRS to WGS84 (EPSG:4326)\n",
    "    main_gdf = gpd.GeoDataFrame(\n",
    "        main_df,\n",
    "        geometry=gpd.points_from_xy(main_df.longitude, main_df.latitude),\n",
    "        crs=\"EPSG:4326\"  # Assuming WGS84 coordinate system\n",
    "    )\n",
    "\n",
    "    # Validate and clean geometries\n",
    "    # main_gdf['geometry'] = main_gdf['geometry'].buffer(0)\n",
    "    # flood_gdf['geometry'] = flood_gdf['geometry'].buffer(0)\n",
    "    \n",
    "    main_gdf = clean_geometries(main_gdf)\n",
    "    flood_gdf = clean_geometries(flood_gdf)\n",
    "\n",
    "    # Simplify geometries to speed up processing\n",
    "    # flood_gdf['geometry'] = flood_gdf['geometry'].simplify(tolerance=0.001)\n",
    "\n",
    "    # Ensure spatial index is built\n",
    "    if not flood_gdf.sindex:\n",
    "        flood_gdf.sindex\n",
    "\n",
    "    # Initialize the 'flood_proximity' column with the highest category (No Risk)\n",
    "    main_gdf['flood_proximity'] = 5\n",
    "\n",
    "    # Update the proximity categories based on spatial relationships\n",
    "    # 1. 'In Flood Area' (1)\n",
    "    main_gdf.loc[flood_gdf.contains(\n",
    "        main_gdf['geometry']).any(), 'flood_proximity'] = 1\n",
    "\n",
    "    # 2. 'High Risk' (2): Within 0.005 degrees and not in the flood area\n",
    "    buffer_gdf = flood_gdf.copy()\n",
    "    buffer_gdf['geometry'] = buffer_gdf.buffer(0.005)\n",
    "    main_gdf.loc[(main_gdf['flood_proximity'] == 5) &\n",
    "                 (gpd.sjoin(main_gdf, buffer_gdf, how='left',\n",
    "                  predicate='intersects')['index_right'].notnull()),\n",
    "                 'flood_proximity'] = 2\n",
    "\n",
    "    # 3. 'Moderate Risk' (3): Within 0.01 degrees and not in the higher risk areas\n",
    "    buffer_gdf['geometry'] = flood_gdf.buffer(0.01)\n",
    "    main_gdf.loc[(main_gdf['flood_proximity'] == 5) &\n",
    "                 (gpd.sjoin(main_gdf, buffer_gdf, how='left',\n",
    "                  predicate='intersects')['index_right'].notnull()),\n",
    "                 'flood_proximity'] = 3\n",
    "\n",
    "    # 4. 'Low Risk' (4): Within 0.02 degrees and not in the higher risk areas\n",
    "    buffer_gdf['geometry'] = flood_gdf.buffer(0.02)\n",
    "    main_gdf.loc[(main_gdf['flood_proximity'] == 5) &\n",
    "                 (gpd.sjoin(main_gdf, buffer_gdf, how='left',\n",
    "                  predicate='intersects')['index_right'].notnull()),\n",
    "                 'flood_proximity'] = 4\n",
    "\n",
    "    # Convert back to a DataFrame if needed\n",
    "    final_df = pd.DataFrame(main_gdf.drop(columns=['geometry']))\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_data_with_flood_proximity = categorize_flood_proximity(\n",
    "    final_merged_data_with_crime_density, flood_data_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>transfer_date</th>\n",
       "      <th>property_type</th>\n",
       "      <th>is_old_or_new</th>\n",
       "      <th>property_tenure</th>\n",
       "      <th>town</th>\n",
       "      <th>district</th>\n",
       "      <th>ppd_transaction_category</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>robbery_density</th>\n",
       "      <th>drugs_density</th>\n",
       "      <th>public_disorder_and_weapons_density</th>\n",
       "      <th>criminal_damage_and_arson_density</th>\n",
       "      <th>theft_from_the_person_density</th>\n",
       "      <th>violence_and_sexual_offences_density</th>\n",
       "      <th>possession_of_weapons_density</th>\n",
       "      <th>public_order_density</th>\n",
       "      <th>bicycle_theft_density</th>\n",
       "      <th>flood_risk_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Flats/Maisonettes</td>\n",
       "      <td>Old</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>Prenton</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.038801</td>\n",
       "      <td>53.384498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Flats/Maisonettes</td>\n",
       "      <td>Old</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>Prenton</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.038801</td>\n",
       "      <td>53.384498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Flats/Maisonettes</td>\n",
       "      <td>Old</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>Prenton</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.038801</td>\n",
       "      <td>53.384498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Flats/Maisonettes</td>\n",
       "      <td>Old</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>Prenton</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.038801</td>\n",
       "      <td>53.384498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Flats/Maisonettes</td>\n",
       "      <td>Old</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>Prenton</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.038801</td>\n",
       "      <td>53.384498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     price transfer_date      property_type is_old_or_new property_tenure  \\\n",
       "0  75000.0    2013-01-02  Flats/Maisonettes           Old       Leasehold   \n",
       "1  75000.0    2013-01-02  Flats/Maisonettes           Old       Leasehold   \n",
       "2  75000.0    2013-01-02  Flats/Maisonettes           Old       Leasehold   \n",
       "3  75000.0    2013-01-02  Flats/Maisonettes           Old       Leasehold   \n",
       "4  75000.0    2013-01-02  Flats/Maisonettes           Old       Leasehold   \n",
       "\n",
       "      town district ppd_transaction_category  longitude   latitude  ...  \\\n",
       "0  Prenton   Wirral                        A  -3.038801  53.384498  ...   \n",
       "1  Prenton   Wirral                        A  -3.038801  53.384498  ...   \n",
       "2  Prenton   Wirral                        A  -3.038801  53.384498  ...   \n",
       "3  Prenton   Wirral                        A  -3.038801  53.384498  ...   \n",
       "4  Prenton   Wirral                        A  -3.038801  53.384498  ...   \n",
       "\n",
       "   robbery_density  drugs_density  public_disorder_and_weapons_density  \\\n",
       "0              0.0            0.0                                  0.0   \n",
       "1              0.0            0.0                                  0.0   \n",
       "2              0.0            0.0                                  0.0   \n",
       "3              0.0            0.0                                  0.0   \n",
       "4              0.0            0.0                                  0.0   \n",
       "\n",
       "   criminal_damage_and_arson_density  theft_from_the_person_density  \\\n",
       "0                                0.0                            0.0   \n",
       "1                                0.0                            0.0   \n",
       "2                                0.0                            0.0   \n",
       "3                                0.0                            0.0   \n",
       "4                                0.0                            0.0   \n",
       "\n",
       "   violence_and_sexual_offences_density  possession_of_weapons_density   \\\n",
       "0                                   0.0                             0.0   \n",
       "1                                   0.0                             0.0   \n",
       "2                                   0.0                             0.0   \n",
       "3                                   0.0                             0.0   \n",
       "4                                   0.0                             0.0   \n",
       "\n",
       "   public_order_density  bicycle_theft_density  flood_risk_category  \n",
       "0                   0.0                    0.0                  5.0  \n",
       "1                   0.0                    0.0                  5.0  \n",
       "2                   0.0                    0.0                  5.0  \n",
       "3                   0.0                    0.0                  5.0  \n",
       "4                   0.0                    0.0                  5.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_final_dataset(df):\n",
    "    \"\"\"\n",
    "    Clean the final merged dataset by handling missing values, dropping unnecessary columns, \n",
    "    and renaming columns for clarity.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The final merged DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop unnecessary or duplicate columns\n",
    "    df.drop(columns=[\n",
    "        'area_name_x', 'area_name_y', \n",
    "        'transaction_id', 'year', 'area_code_x', 'area_code_y', 'date',\n",
    "        'region_name', 'area_code', 'county'\n",
    "    ], inplace=True, errors='ignore')\n",
    "\n",
    "    # Fill missing flood proximity with 0 (No Risk) and crime density with 0\n",
    "    df['flood_proximity'].fillna(0, inplace=True)\n",
    "    for col in df.columns:\n",
    "        if 'density' in col:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "\n",
    "    # Rename columns for clarity if necessary\n",
    "    df.rename(columns={\n",
    "        'Other crime_density': 'other_crime_density',\n",
    "        'Anti-social behaviour_density': 'anti-social_behaviour_density',\n",
    "        'Violent crime_density': 'violent_crime_density ',\n",
    "        'Vehicle crime_density': 'vehicle_crime_density',\n",
    "        'Shoplifting_density': 'shoplifting_density',\n",
    "        'Other theft_density': 'other_theft_density',\n",
    "        'Burglary_density ': 'burglary_density ',\n",
    "        'Robbery_density': 'robbery_density',\n",
    "        'Drugs_density': 'drugs_density',\n",
    "        'Public disorder and weapons_density': 'public_disorder_and_weapons_density',\n",
    "        'Criminal damage and arson_density': 'criminal_damage_and_arson_density',\n",
    "        'Theft from the person_density': 'theft_from_the_person_density',\n",
    "        'Violence and sexual offences_density': 'violence_and_sexual_offences_density',\n",
    "        'Possession of weapons_density': 'possession_of_weapons_density ',\n",
    "        'Public order_density': 'public_order_density',\n",
    "        'Bicycle theft_density': 'bicycle_theft_density',\n",
    "        'flood_proximity': 'flood_risk_category'\n",
    "    }, inplace=True)\n",
    "\n",
    "\n",
    "    # Remove duplicate rows if any\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # drop null row with missing date\n",
    "    df = df.drop(\n",
    "        df.tail(1).index)\n",
    "\n",
    "    # Reset index if needed\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the cleaning function to the final merged data\n",
    "cleaned_final_df = clean_final_dataset(final_merged_data_with_flood_proximity)\n",
    "\n",
    "# Display the first few rows to verify the cleaning process\n",
    "cleaned_final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381037, 34)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension of cleaned data\n",
    "cleaned_final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 381037 entries, 0 to 381036\n",
      "Data columns (total 34 columns):\n",
      " #   Column                                Non-Null Count   Dtype         \n",
      "---  ------                                --------------   -----         \n",
      " 0   price                                 381037 non-null  float64       \n",
      " 1   transfer_date                         381037 non-null  datetime64[ns]\n",
      " 2   property_type                         381037 non-null  category      \n",
      " 3   is_old_or_new                         381037 non-null  category      \n",
      " 4   property_tenure                       381037 non-null  category      \n",
      " 5   town                                  381037 non-null  object        \n",
      " 6   district                              381037 non-null  object        \n",
      " 7   ppd_transaction_category              381037 non-null  category      \n",
      " 8   longitude                             381037 non-null  float64       \n",
      " 9   latitude                              381037 non-null  float64       \n",
      " 10  employment_rate                       381037 non-null  float64       \n",
      " 11  average_price                         381037 non-null  float64       \n",
      " 12  index                                 381037 non-null  float64       \n",
      " 13  1m%_change                            381037 non-null  float64       \n",
      " 14  12m%_change                           381037 non-null  float64       \n",
      " 15  sales_volume                          381037 non-null  float64       \n",
      " 16  gross_median_weekly_pay               381037 non-null  float64       \n",
      " 17  other_crime_density                   381037 non-null  float64       \n",
      " 18  anti-social_behaviour_density         381037 non-null  float64       \n",
      " 19  violent_crime_density                 381037 non-null  float64       \n",
      " 20  vehicle_crime_density                 381037 non-null  float64       \n",
      " 21  shoplifting_density                   381037 non-null  float64       \n",
      " 22  other_theft_density                   381037 non-null  float64       \n",
      " 23  Burglary_density                      381037 non-null  float64       \n",
      " 24  robbery_density                       381037 non-null  float64       \n",
      " 25  drugs_density                         381037 non-null  float64       \n",
      " 26  public_disorder_and_weapons_density   381037 non-null  float64       \n",
      " 27  criminal_damage_and_arson_density     381037 non-null  float64       \n",
      " 28  theft_from_the_person_density         381037 non-null  float64       \n",
      " 29  violence_and_sexual_offences_density  381037 non-null  float64       \n",
      " 30  possession_of_weapons_density         381037 non-null  float64       \n",
      " 31  public_order_density                  381037 non-null  float64       \n",
      " 32  bicycle_theft_density                 381037 non-null  float64       \n",
      " 33  flood_risk_category                   381037 non-null  float64       \n",
      "dtypes: category(4), datetime64[ns](1), float64(27), object(2)\n",
      "memory usage: 88.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# print dataframe information\n",
    "cleaned_final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    381037.000000\n",
       "mean     155901.919047\n",
       "std       76327.859566\n",
       "min       50000.000000\n",
       "25%       96500.000000\n",
       "50%      140000.000000\n",
       "75%      196250.000000\n",
       "max      400000.000000\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_final_df.price.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly variable price data with a standard deviation in range of 10^5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create box plot to view outliers\n",
    "sns.boxplot(x=cleaned_final_df['price'])\n",
    "plt.title('Boxplot of Property Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
