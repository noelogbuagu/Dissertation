{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Property Prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "import requests\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant area codes\n",
    "area_codes = [\n",
    "    'E08000011', 'E08000012', 'E11000002', 'E08000014', 'E08000013',\n",
    "    'E08000007', 'E06000007', 'E08000010', 'E08000015'\n",
    "]\n",
    "\n",
    "# Regions in Merseyside\n",
    "regions = [\n",
    "    'Prenton', 'Newton-Le-Willows', 'Birkenhead',\n",
    "    'Wirral', 'Bootle', 'St Helens', 'Wallasey', 'Southport',\n",
    "    'Prescot', 'Wigan', 'Widnes', 'Neston', 'Warrington',\n",
    "    'Ellesmere Port', 'Wilmslow', 'Coniston', 'Stockport', 'Northwood',\n",
    "    'Crewe', 'Winsford', 'Merseyside', 'Sefton', 'Wirral', 'Liverpool', 'Knowsley'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_column(df, column_name, mapping, remove_values=None, new_dtype='category'):\n",
    "    \"\"\"\n",
    "    Standardise a DataFrame column by applying a mapping, optionally removing specific values, \n",
    "    and changing the data type.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    column_name (str): The name of the column to standardize.\n",
    "    mapping (dict): A dictionary mapping old values to new values.\n",
    "    remove_values (list, optional): A list of values to remove from the column. Default is None.\n",
    "    new_dtype (str, optional): The new data type for the column. Default is 'category'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with the standardized column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Optionally remove rows with specific values\n",
    "    if remove_values:\n",
    "        df = df[~df[column_name].isin(remove_values)]\n",
    "\n",
    "    # Apply the mapping to replace old values with new values\n",
    "    df[column_name] = df[column_name].replace(mapping).astype(new_dtype)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, new_column_names):\n",
    "    \"\"\"\n",
    "    Rename the columns of a DataFrame using a provided list of new column names.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame whose columns need to be renamed.\n",
    "    new_column_names (list): A list of new column names.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    # Check if the number of new column names matches the number of columns in the DataFrame\n",
    "    if len(new_column_names) != df.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"The number of new column names must match the number of columns in the DataFrame.\")\n",
    "\n",
    "    # Rename the columns\n",
    "    df.columns = new_column_names\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices Paid Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_lookup_postcodes(postcodes, batch_size=100):\n",
    "    \"\"\"\n",
    "    Perform a bulk lookup for postcodes using the postcodes.io API.\n",
    "\n",
    "    Parameters:\n",
    "    postcodes (list): A list of postcodes to lookup.\n",
    "    batch_size (int): The number of postcodes to include in each batch (max 100).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary mapping postcodes to their respective data, including latitude, longitude, and termination data if applicable.\n",
    "    \"\"\"\n",
    "    url = \"https://api.postcodes.io/postcodes\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    results = {}\n",
    "\n",
    "    # Process the postcodes in batches\n",
    "    for i in range(0, len(postcodes), batch_size):\n",
    "        batch = postcodes[i:i+batch_size]\n",
    "        data = {\"postcodes\": batch}\n",
    "\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            for result in response_data['result']:\n",
    "                postcode = result['query']\n",
    "                if result['result'] is not None:\n",
    "                    results[postcode] = {\n",
    "                        'longitude': result['result']['longitude'],\n",
    "                        'latitude': result['result']['latitude'],\n",
    "                        'is_terminated': False\n",
    "                    }\n",
    "                else:\n",
    "                    # If the postcode is terminated, set to None and mark as terminated\n",
    "                    results[postcode] = {\n",
    "                        'longitude': None,\n",
    "                        'latitude': None,\n",
    "                        'is_terminated': True\n",
    "                    }\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to retrieve data for batch starting at {i}: {response.status_code}\")\n",
    "        time.sleep(0.1)  # To avoid rate limiting\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_terminated_postcodes(postcode_data):\n",
    "    \"\"\"\n",
    "    Handle postcodes that are marked as terminated by checking the terminated postcode API.\n",
    "\n",
    "    Parameters:\n",
    "    postcode_data (dict): A dictionary of postcodes with their data, including termination status.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated dictionary with longitude and latitude for terminated postcodes if available.\n",
    "    \"\"\"\n",
    "    terminated_postcodes = [postcode for postcode,\n",
    "                            data in postcode_data.items() if data['is_terminated']]\n",
    "\n",
    "    if not terminated_postcodes:\n",
    "        return postcode_data  # No terminated postcodes to handle\n",
    "\n",
    "    # Query terminated postcodes in bulk (max 100 at a time)\n",
    "    url = \"https://api.postcodes.io/terminated_postcodes\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    batch_size = 100\n",
    "\n",
    "    for i in range(0, len(terminated_postcodes), batch_size):\n",
    "        batch = terminated_postcodes[i:i+batch_size]\n",
    "        data = {\"postcodes\": batch}\n",
    "\n",
    "        response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            for result in response_data['result']:\n",
    "                postcode = result['query']\n",
    "                if result['result'] is not None:\n",
    "                    postcode_data[postcode]['longitude'] = result['result']['longitude']\n",
    "                    postcode_data[postcode]['latitude'] = result['result']['latitude']\n",
    "                else:\n",
    "                    # If still no data, keep as None\n",
    "                    print(f\"No data found for terminated postcode: {postcode}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to retrieve data for terminated postcodes batch starting at {i}: {response.status_code}\")\n",
    "        time.sleep(0.1)  # To avoid rate limiting\n",
    "\n",
    "    return postcode_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_long_lat_columns(df, postcode_column='postcode'):\n",
    "    \"\"\"\n",
    "    Add longitude and latitude columns to the DataFrame based on the postcode column using Bulk Lookup.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame with a postcode column.\n",
    "    postcode_column (str): The name of the column containing postcodes. Default is 'postcode'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with added 'longitude' and 'latitude' columns.\n",
    "    \"\"\"\n",
    "    postcodes = df[postcode_column].unique().tolist()\n",
    "    postcode_data = bulk_lookup_postcodes(postcodes)\n",
    "    postcode_data = handle_terminated_postcodes(postcode_data)\n",
    "\n",
    "    # Map the longitude and latitude back to the original DataFrame\n",
    "    df['longitude'] = df[postcode_column].map(\n",
    "        lambda x: postcode_data[x]['longitude'])\n",
    "    df['latitude'] = df[postcode_column].map(\n",
    "        lambda x: postcode_data[x]['latitude'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for prices paid data\n",
    "def wrangle_prices_paid(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # drop complete duplicates from house_data\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # add column names to df\n",
    "    column_names = [\n",
    "        'transaction_id', 'price', 'transfer_date',\n",
    "        'postcode', 'property_type', 'is_old_or_new',\n",
    "        'property_tenure', 'house_number_or_name', 'unit_number',\n",
    "        'street', 'locality', 'town', 'district', 'county',\n",
    "        'ppd_transaction_category', 'record_status_monthly_file_only'\n",
    "    ]\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # convert transfer date to datetime\n",
    "    df['transfer_date'] = pd.to_datetime(df['transfer_date'])\n",
    "    \n",
    "    # sort data by teansaction date\n",
    "    df = df.sort_values('transfer_date', ascending=True)\n",
    "    \n",
    "    # filter data for freehold transactions in merseyside from 2013 to 2023\n",
    "    df = df[(df['county'] == 'MERSEYSIDE') & (\n",
    "        df['transfer_date'].dt.year >= 2013) & (df['transfer_date'].dt.year <= 2023)]\n",
    "\n",
    "    # convert price to float type\n",
    "    df['price'] = df['price'].astype(float)\n",
    "    \n",
    "    # remove outliers in price by values in the bottom and top 5% of properties\n",
    "    # low, high = df['price'].quantile([0.05, 0.95])\n",
    "    # mask_area = df['price'].between(low, high)\n",
    "    # df = df[mask_area]\n",
    "\n",
    "    # convert ppd_transaction_category to category type\n",
    "    df['ppd_transaction_category'] = df['ppd_transaction_category'].astype(\n",
    "        'category')\n",
    "\n",
    "    # define mappings for replacement\n",
    "    property_type_mapping = {'T': 'Terraced', 'D': 'Detached', 'F': 'Flats/Maisonettes',\n",
    "                             'S': 'Semi-Detached', 'O': 'Other'}\n",
    "    old_or_new_mapping = {'N': 'Old', 'Y': 'New'}\n",
    "    property_tenure_mapping = {'F': 'Freehold', 'L': 'Leasehold'}\n",
    "\n",
    "    # standardize 'property_type' column\n",
    "    df = standardise_column(df, 'property_type', property_type_mapping)\n",
    "    # standardize 'is_old_or_new' column\n",
    "    df = standardise_column(df, 'is_old_or_new', old_or_new_mapping)\n",
    "    # standardize 'property_tenure' column and remove rows with 'U' before standardising\n",
    "    df = standardise_column(df, 'property_tenure',\n",
    "                            property_tenure_mapping, remove_values=['U'])\n",
    "\n",
    "    # convert capital case columns to title case\n",
    "    df['town'] = df['town'].str.title()\n",
    "    df['district'] = df['district'].str.title()\n",
    "    df['county'] = df['county'].str.title()\n",
    "    \n",
    "    # exclude rows with null postcode values\n",
    "    df = df[~df['postcode'].isnull()]\n",
    "    \n",
    "    # created latitude and longitude columns\n",
    "    df = add_long_lat_columns(df)\n",
    "\n",
    "    # drop redundant columns\n",
    "    df.drop(columns=['house_number_or_name', 'unit_number', 'locality',\n",
    "                     'street', 'record_status_monthly_file_only', 'postcode'],\n",
    "            inplace=True\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data for terminated postcodes batch starting at 0: 404\n",
      "Failed to retrieve data for terminated postcodes batch starting at 100: 404\n",
      "Failed to retrieve data for terminated postcodes batch starting at 200: 404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>price</th>\n",
       "      <th>transfer_date</th>\n",
       "      <th>property_type</th>\n",
       "      <th>is_old_or_new</th>\n",
       "      <th>property_tenure</th>\n",
       "      <th>town</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>ppd_transaction_category</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18351985</th>\n",
       "      <td>{EF89E3A8-2BD1-4347-8B9B-F2CEEB2E62DC}</td>\n",
       "      <td>75000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Flats/Maisonettes</td>\n",
       "      <td>Old</td>\n",
       "      <td>Leasehold</td>\n",
       "      <td>Prenton</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.038801</td>\n",
       "      <td>53.384498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18251630</th>\n",
       "      <td>{CD1FD346-02E2-40B9-AD20-AF02A78999D1}</td>\n",
       "      <td>113000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Semi-Detached</td>\n",
       "      <td>Old</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Sefton</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-2.946158</td>\n",
       "      <td>53.519373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18458777</th>\n",
       "      <td>{3008D681-978E-4FB2-B152-5077F80C64CF}</td>\n",
       "      <td>79500.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Terraced</td>\n",
       "      <td>Old</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>Birkenhead</td>\n",
       "      <td>Wirral</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-3.019912</td>\n",
       "      <td>53.376715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18512520</th>\n",
       "      <td>{554C7E6D-FB60-4BF3-AEF0-F18802D4C110}</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Detached</td>\n",
       "      <td>Old</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>Newton-Le-Willows</td>\n",
       "      <td>St Helens</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-2.635595</td>\n",
       "      <td>53.479038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18275462</th>\n",
       "      <td>{3A27DE8C-0D42-41CC-8501-7367F7E98993}</td>\n",
       "      <td>115000.0</td>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>Semi-Detached</td>\n",
       "      <td>Old</td>\n",
       "      <td>Freehold</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>A</td>\n",
       "      <td>-2.904433</td>\n",
       "      <td>53.412073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  transaction_id     price transfer_date  \\\n",
       "18351985  {EF89E3A8-2BD1-4347-8B9B-F2CEEB2E62DC}   75000.0    2013-01-02   \n",
       "18251630  {CD1FD346-02E2-40B9-AD20-AF02A78999D1}  113000.0    2013-01-02   \n",
       "18458777  {3008D681-978E-4FB2-B152-5077F80C64CF}   79500.0    2013-01-02   \n",
       "18512520  {554C7E6D-FB60-4BF3-AEF0-F18802D4C110}  385000.0    2013-01-02   \n",
       "18275462  {3A27DE8C-0D42-41CC-8501-7367F7E98993}  115000.0    2013-01-02   \n",
       "\n",
       "              property_type is_old_or_new property_tenure               town  \\\n",
       "18351985  Flats/Maisonettes           Old       Leasehold            Prenton   \n",
       "18251630      Semi-Detached           Old        Freehold          Liverpool   \n",
       "18458777           Terraced           Old        Freehold         Birkenhead   \n",
       "18512520           Detached           Old        Freehold  Newton-Le-Willows   \n",
       "18275462      Semi-Detached           Old        Freehold          Liverpool   \n",
       "\n",
       "           district      county ppd_transaction_category  longitude   latitude  \n",
       "18351985     Wirral  Merseyside                        A  -3.038801  53.384498  \n",
       "18251630     Sefton  Merseyside                        A  -2.946158  53.519373  \n",
       "18458777     Wirral  Merseyside                        A  -3.019912  53.376715  \n",
       "18512520  St Helens  Merseyside                        A  -2.635595  53.479038  \n",
       "18275462  Liverpool  Merseyside                        A  -2.904433  53.412073  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the prices paid data\n",
    "prices_paid_data_df = wrangle_prices_paid(\n",
    "    filepath=\"raw_data/prices_paid.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "prices_paid_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Employment Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for employment data\n",
    "def wrangle_employment(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # add column names to df\n",
    "    column_names = ['area_code', 'area_name', 'year', 'employment_rate',\n",
    "                    'confidence_interval_lower', 'confidence_interval_upper']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # filter for employment rate from 2013 to 2023\n",
    "    df = df[(df['year'] >= 2013) & (df['year'] <= 2023)]\n",
    "\n",
    "    # relevant columns\n",
    "    relevant_columns = ['area_code', 'area_name', 'year', 'employment_rate']\n",
    "    # select relevant columns\n",
    "    df = df[relevant_columns]\n",
    "\n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['area_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('year')\n",
    "    \n",
    "    # Convert the year column to string, then to datetime\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['year'] = pd.to_datetime(\n",
    "        df['year'], format='%Y')\n",
    "    # to keep only the year part\n",
    "    df['year'] = df['year'].dt.year\n",
    "        \n",
    "    # Convert employment rate from percentatage to rates\n",
    "    df['employment_rate'] = df['employment_rate']/100\n",
    "    \n",
    "    # aggregate the data\n",
    "    df = df.groupby(['area_code', 'area_name', 'year']).agg({\n",
    "        'employment_rate': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_code</th>\n",
       "      <th>area_name</th>\n",
       "      <th>year</th>\n",
       "      <th>employment_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   area_code   area_name  year  employment_rate\n",
       "0  E06000007  Warrington  2013            0.782\n",
       "1  E06000007  Warrington  2014            0.779\n",
       "2  E06000007  Warrington  2015            0.779\n",
       "3  E06000007  Warrington  2016            0.762\n",
       "4  E06000007  Warrington  2017            0.774"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the employment data\n",
    "employment_data_df = wrangle_employment(filepath=\"raw_data/employment_data.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "employment_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UK HPI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for hpi data\n",
    "def wrangle_hpi(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # select relevant columns\n",
    "    relevant_columns = [\n",
    "        'Date', 'RegionName', 'AreaCode', 'AveragePrice', 'Index', '1m%Change', '12m%Change', 'SalesVolume'\n",
    "    ]\n",
    "    df = df[relevant_columns]\n",
    "    \n",
    "    # change column names\n",
    "    column_names = ['date', 'region_name', 'area_code', 'average_price',\n",
    "                    'index', '1m%_change', '12m%_change', 'sales_volume']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    # change date to dattime instead of object\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Filter for the date range\n",
    "    df = df[(df['date'] >= '2013-01-01') & (\n",
    "        df['date'] <= '2023-12-31')]\n",
    "    \n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['region_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('date')\n",
    "    \n",
    "    # drop complete duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>region_name</th>\n",
       "      <th>area_code</th>\n",
       "      <th>average_price</th>\n",
       "      <th>index</th>\n",
       "      <th>1m%_change</th>\n",
       "      <th>12m%_change</th>\n",
       "      <th>sales_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86092</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Knowsley</td>\n",
       "      <td>E08000011</td>\n",
       "      <td>105390.6729</td>\n",
       "      <td>97.958916</td>\n",
       "      <td>0.304925</td>\n",
       "      <td>-1.857656</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86105</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>E08000012</td>\n",
       "      <td>105140.5433</td>\n",
       "      <td>93.986552</td>\n",
       "      <td>-0.314031</td>\n",
       "      <td>-2.003400</td>\n",
       "      <td>292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86115</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Merseyside</td>\n",
       "      <td>E11000002</td>\n",
       "      <td>118307.7900</td>\n",
       "      <td>95.814439</td>\n",
       "      <td>-0.183908</td>\n",
       "      <td>-1.070033</td>\n",
       "      <td>891.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86196</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>Sefton</td>\n",
       "      <td>E08000014</td>\n",
       "      <td>135940.7468</td>\n",
       "      <td>98.603056</td>\n",
       "      <td>0.431996</td>\n",
       "      <td>-0.460236</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86225</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>St Helens</td>\n",
       "      <td>E08000013</td>\n",
       "      <td>107387.2971</td>\n",
       "      <td>96.951534</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.660812</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date region_name  area_code  average_price      index  1m%_change  \\\n",
       "86092 2013-01-01    Knowsley  E08000011    105390.6729  97.958916    0.304925   \n",
       "86105 2013-01-01   Liverpool  E08000012    105140.5433  93.986552   -0.314031   \n",
       "86115 2013-01-01  Merseyside  E11000002    118307.7900  95.814439   -0.183908   \n",
       "86196 2013-01-01      Sefton  E08000014    135940.7468  98.603056    0.431996   \n",
       "86225 2013-01-01   St Helens  E08000013    107387.2971  96.951534    0.002608   \n",
       "\n",
       "       12m%_change  sales_volume  \n",
       "86092    -1.857656          67.0  \n",
       "86105    -2.003400         292.0  \n",
       "86115    -1.070033         891.0  \n",
       "86196    -0.460236         176.0  \n",
       "86225     0.660812         103.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the HPI data\n",
    "hpi_data_df = wrangle_hpi(filepath=\"raw_data/uk_hpi.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "hpi_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Income Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrangle function for hpi data\n",
    "def wrangle_income(filepath):\n",
    "\n",
    "    # import file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # change column names\n",
    "    column_names = ['area_code', 'area_name', 'year',\n",
    "                        'gross_median_weekly_pay', 'confidence_interval_lower', 'confidence_interval_upper']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "\n",
    "    # relevant columns\n",
    "    relevant_columns = ['area_code', 'area_name',\n",
    "                        'year', 'gross_median_weekly_pay']\n",
    "    # select relevant columns\n",
    "    df = df[relevant_columns]\n",
    "    \n",
    "    \n",
    "    # Convert the year column to string, then to datetime\n",
    "    df['year'] = df['year'].astype(str)\n",
    "    df['year'] = pd.to_datetime(\n",
    "        df['year'], format='%Y')\n",
    "\n",
    "    # to keep only the year part\n",
    "    df['year'] = df['year'].dt.year\n",
    "\n",
    "    # filter for relevant area codes and names\n",
    "    df = df[(df['area_name'].isin(\n",
    "        regions)) | (df['area_code'].isin(\n",
    "            area_codes))].sort_values('year')\n",
    "    \n",
    "    # aggregate the data\n",
    "    df = df.groupby(['area_code', 'area_name', 'year']).agg({\n",
    "        'gross_median_weekly_pay': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # drop rows with any missing values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area_code</th>\n",
       "      <th>area_name</th>\n",
       "      <th>year</th>\n",
       "      <th>gross_median_weekly_pay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2008</td>\n",
       "      <td>410.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2009</td>\n",
       "      <td>424.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2010</td>\n",
       "      <td>428.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2011</td>\n",
       "      <td>402.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E06000007</td>\n",
       "      <td>Warrington</td>\n",
       "      <td>2012</td>\n",
       "      <td>411.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   area_code   area_name  year  gross_median_weekly_pay\n",
       "0  E06000007  Warrington  2008                    410.2\n",
       "1  E06000007  Warrington  2009                    424.2\n",
       "2  E06000007  Warrington  2010                    428.5\n",
       "3  E06000007  Warrington  2011                    402.7\n",
       "4  E06000007  Warrington  2012                    411.8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the income data\n",
    "income_data_df = wrangle_income(filepath= \"raw_data/income_data.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "income_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_polygons(large_polygon, divisions=3):\n",
    "    \"\"\"\n",
    "    Divide a large polygon into smaller sub-polygons.\n",
    "\n",
    "    Parameters:\n",
    "    large_polygon (list): A list of coordinates defining the large polygon.\n",
    "    divisions (int): The number of divisions along each axis.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of sub-polygons.\n",
    "    \"\"\"\n",
    "    lat_min, lat_max = min(pt[0] for pt in large_polygon), max(\n",
    "        pt[0] for pt in large_polygon)\n",
    "    lng_min, lng_max = min(pt[1] for pt in large_polygon), max(\n",
    "        pt[1] for pt in large_polygon)\n",
    "\n",
    "    lat_step = (lat_max - lat_min) / divisions\n",
    "    lng_step = (lng_max - lng_min) / divisions\n",
    "\n",
    "    sub_polygons = []\n",
    "    for i in range(divisions):\n",
    "        for j in range(divisions):\n",
    "            sub_polygon = [\n",
    "                [lat_min + i * lat_step, lng_min + j * lng_step],\n",
    "                [lat_min + i * lat_step, lng_min + (j + 1) * lng_step],\n",
    "                [lat_min + (i + 1) * lat_step, lng_min + (j + 1) * lng_step],\n",
    "                [lat_min + (i + 1) * lat_step, lng_min + j * lng_step],\n",
    "                [lat_min + i * lat_step, lng_min + j * lng_step],\n",
    "            ]\n",
    "            sub_polygons.append(sub_polygon)\n",
    "\n",
    "    return sub_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_crime_data(sub_polygons, start_date, end_date, endpoint=\"https://data.police.uk/api/crimes-street/all-crime\"):\n",
    "    \"\"\"\n",
    "    Fetch crime data from the Police API for a set of sub-polygons over a date range.\n",
    "\n",
    "    Parameters:\n",
    "    sub_polygons (list): A list of sub-polygons.\n",
    "    start_date (datetime): The start date for fetching data.\n",
    "    end_date (datetime): The end date for fetching data.\n",
    "    endpoint (str): The API endpoint for fetching crime data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the collected crime data.\n",
    "    \"\"\"\n",
    "    all_crimes = []\n",
    "\n",
    "    for sub_polygon in sub_polygons:\n",
    "        polygon_str = \":\".join([f\"{lat},{lng}\" for lat, lng in sub_polygon])\n",
    "\n",
    "        current_date = start_date\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime(\"%Y-%m\")\n",
    "\n",
    "            # Make the API call using the sub-polygon\n",
    "            api_url = f\"{endpoint}?date={date_str}&poly={polygon_str}\"\n",
    "\n",
    "            # Send the request to the API\n",
    "            response = requests.get(api_url)\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                crimes = response.json()\n",
    "                all_crimes.extend(crimes)\n",
    "            elif response.status_code == 503:\n",
    "                print(\n",
    "                    f\"Request exceeded limit for {date_str} in sub-polygon\"\n",
    "                )\n",
    "            else:\n",
    "                print (\n",
    "                    f\"Failed to retrieve data for {date_str}: {response.status_code}\"\n",
    "                )\n",
    "                \n",
    "            # Move to the next month\n",
    "            current_date += timedelta(days=31)\n",
    "            # Ensure we start at the beginning of the next month\n",
    "            current_date = current_date.replace(day=1)\n",
    "\n",
    "    # Convert the collected data to a DataFrame\n",
    "    df = pd.DataFrame(all_crimes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_crime_data(df):\n",
    "    \"\"\"\n",
    "    Clean and extract relevant fields from the crime data DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing raw crime data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A cleaned DataFrame with only relevant fields.\n",
    "    \"\"\"\n",
    "    # Extract latitude and longitude from the 'location' dictionary\n",
    "    df['latitude'] = df['location'].apply(\n",
    "        lambda x: x['latitude'])\n",
    "    df['longitude'] = df['location'].apply(\n",
    "        lambda x: x['longitude'])\n",
    "\n",
    "    # Extract the 'id' and 'name' from the 'street' dictionary within the 'location' dictionary\n",
    "    df['street_id'] = df['location'].apply(\n",
    "        lambda x: x['street']['id'])\n",
    "    df['street_name'] = df['location'].apply(\n",
    "        lambda x: x['street']['name'])\n",
    "\n",
    "    # Extract 'category' and 'date' from the 'outcome_status' dictionary\n",
    "    df['outcome_category'] = df['outcome_status'].apply(\n",
    "        lambda x: x['category'] if pd.notnull(x) else None)\n",
    "    df['outcome_date'] = df['outcome_status'].apply(\n",
    "        lambda x: x['date'] if pd.notnull(x) else None)\n",
    "\n",
    "    # Drop the original 'location' and 'outcome_status' columns if they're no longer needed\n",
    "    df.drop(columns=['location', 'outcome_status'], inplace=True)\n",
    "\n",
    "    # Extract only relevant columns for crime location and category\n",
    "    df = df[[\n",
    "        'month', 'category', 'latitude', 'longitude']]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_crime(large_polygon, divisions=3, start_date=datetime(2021, 6, 1), end_date=datetime(2023, 12, 31)):\n",
    "    \"\"\"\n",
    "    Complete wrangling process for crime data, including fetching and cleaning.\n",
    "\n",
    "    Parameters:\n",
    "    large_polygon (list): A list of coordinates defining the large polygon.\n",
    "    divisions (int): The number of divisions to create sub-polygons. Default is 3.\n",
    "    start_date (datetime): The start date for fetching data. Default is June 2021.\n",
    "    end_date (datetime): The end date for fetching data. Default is he last day of 2023.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A cleaned DataFrame ready for analysis.\n",
    "    \"\"\"\n",
    "    # Divide the large polygon into smaller sub-polygons\n",
    "    sub_polygons = create_sub_polygons(large_polygon, divisions)\n",
    "\n",
    "    # Fetch the crime data\n",
    "    df = fetch_crime_data(sub_polygons, start_date, end_date)\n",
    "\n",
    "    # Clean the crime data\n",
    "    df = clean_crime_data(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data for 2021-06: 404\n",
      "Failed to retrieve data for 2021-06: 404\n",
      "Failed to retrieve data for 2021-06: 404\n",
      "Failed to retrieve data for 2021-06: 404\n",
      "Failed to retrieve data for 2021-06: 404\n",
      "Failed to retrieve data for 2021-06: 404\n",
      "Failed to retrieve data for 2021-06: 404\n",
      "Failed to retrieve data for 2021-06: 404\n",
      "Failed to retrieve data for 2021-06: 404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>category</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-07</td>\n",
       "      <td>anti-social-behaviour</td>\n",
       "      <td>53.384044</td>\n",
       "      <td>-3.049570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-07</td>\n",
       "      <td>anti-social-behaviour</td>\n",
       "      <td>53.404007</td>\n",
       "      <td>-3.116186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-07</td>\n",
       "      <td>anti-social-behaviour</td>\n",
       "      <td>53.403171</td>\n",
       "      <td>-3.058976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-07</td>\n",
       "      <td>anti-social-behaviour</td>\n",
       "      <td>53.422767</td>\n",
       "      <td>-3.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-07</td>\n",
       "      <td>anti-social-behaviour</td>\n",
       "      <td>53.394599</td>\n",
       "      <td>-3.025633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month               category   latitude  longitude\n",
       "0  2021-07  anti-social-behaviour  53.384044  -3.049570\n",
       "1  2021-07  anti-social-behaviour  53.404007  -3.116186\n",
       "2  2021-07  anti-social-behaviour  53.403171  -3.058976\n",
       "3  2021-07  anti-social-behaviour  53.422767  -3.032300\n",
       "4  2021-07  anti-social-behaviour  53.394599  -3.025633"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a large polygon that covers Merseyside\n",
    "large_polygon = [\n",
    "    [53.6967, -3.2603],  # Northwest corner (near Southport)\n",
    "    [53.3700, -3.2603],  # Southwest corner (near Wirral)\n",
    "    [53.3700, -2.5500],  # Southeast corner (near Warrington)\n",
    "    [53.6967, -2.5500],  # Northeast corner (near Wigan)\n",
    "    [53.6967, -3.2603],  # Closing the polygon back at the Northwest corner\n",
    "]\n",
    "\n",
    "# Wrangle the crime data\n",
    "crime_data_df = wrangle_crime(large_polygon)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "crime_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flood Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_flood_data(api_url=\"https://environment.data.gov.uk/flood-monitoring/id/floodAreas?&_limit=5000\"):\n",
    "    \"\"\"\n",
    "    Fetch flood area data from the Environment Agency API.\n",
    "\n",
    "    Parameters:\n",
    "    api_url (str): The API endpoint to fetch flood data. Default is set to the flood areas endpoint.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries containing flood area data.\n",
    "    \"\"\"\n",
    "    response = requests.get(url=api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Convert the response to JSON format and extract relevant data\n",
    "        flood_data = response.json()\n",
    "        flood_areas = flood_data.get('items', [])\n",
    "        return flood_areas\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_flood_data(flood_areas):\n",
    "    \"\"\"\n",
    "    Prepare and clean flood area data by extracting relevant fields and converting to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    flood_areas (list): A list of dictionaries containing flood area data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the prepared flood area data.\n",
    "    \"\"\"\n",
    "    flood_areas_list = []\n",
    "    for area in flood_areas:\n",
    "        flood_areas_list.append({\n",
    "            'county': area.get('county'),\n",
    "            'description': area.get('description'),\n",
    "            'eaAreaName': area.get('eaAreaName'),\n",
    "            'lat': area.get('lat'),\n",
    "            'long': area.get('long'),\n",
    "            'riverOrSea': area.get('riverOrSea'),\n",
    "            'polygon': area.get('polygon')\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(flood_areas_list)\n",
    "\n",
    "    # Rename columns to more descriptive names\n",
    "    column_names = ['county', 'text_description',\n",
    "                    'area_name', 'lat', 'lon', 'water_source', 'polygon']\n",
    "    df = rename_columns(df, column_names)\n",
    "    \n",
    "    df = df[df['area_name'].str.contains(\n",
    "        'mersey', case=False, na=False)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_geojson(uri):\n",
    "    \"\"\"\n",
    "    Fetch and parse GeoJSON data from a polygon URI.\n",
    "\n",
    "    Parameters:\n",
    "    uri (str): The URI pointing to the GeoJSON polygon.\n",
    "\n",
    "    Returns:\n",
    "    geometry: A Shapely geometry object created from the GeoJSON data.\n",
    "    \"\"\"\n",
    "    response = requests.get(uri)\n",
    "    if response.status_code == 200:\n",
    "        geojson = response.json()\n",
    "        # Access the 'geometry' from the first feature\n",
    "        if 'features' in geojson and len(geojson['features']) > 0:\n",
    "            geometry = geojson['features'][0]['geometry']\n",
    "            return shape(geometry)  # Convert GeoJSON to Shapely geometry\n",
    "        else:\n",
    "            print(f\"No features found in GeoJSON data from {uri}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve GeoJSON data from {uri}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_geodataframe(flood_areas_df):\n",
    "    \"\"\"\n",
    "    Convert the DataFrame to a GeoDataFrame by applying the GeoJSON fetching function.\n",
    "\n",
    "    Parameters:\n",
    "    flood_areas_df (DataFrame): The input DataFrame containing flood area data.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: A GeoDataFrame with Shapely geometries for each flood area.\n",
    "    \"\"\"\n",
    "    # Apply the fetch_geojson function to the 'polygon' column\n",
    "    flood_areas_df['geometry'] = flood_areas_df['polygon'].apply(fetch_geojson)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    flood_areas_gdf = gpd.GeoDataFrame(flood_areas_df, geometry='geometry')\n",
    "\n",
    "    return flood_areas_gdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_flood_data():\n",
    "    \"\"\"\n",
    "    Complete wrangling process for flood data, including fetching, cleaning, filtering, and conversion to GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame: A cleaned and filtered GeoDataFrame ready for spatial analysis.\n",
    "    \"\"\"\n",
    "    # Fetch flood data from the API\n",
    "    flood_areas = fetch_flood_data()\n",
    "\n",
    "    # Prepare and clean the data\n",
    "    df = prepare_flood_data(flood_areas)\n",
    "\n",
    "    # Convert to a GeoDataFrame\n",
    "    flood_areas_gdf = convert_to_geodataframe(df)\n",
    "\n",
    "    return flood_areas_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>text_description</th>\n",
       "      <th>area_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>water_source</th>\n",
       "      <th>polygon</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Manchester</td>\n",
       "      <td>Land adjacent to the River Mersey at West Dids...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.41100</td>\n",
       "      <td>-2.24160</td>\n",
       "      <td>River Mersey</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>POLYGON ((-2.24472 53.4153, -2.24508 53.41532,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Cheshire East, Manchester, Salford, Stockport,...</td>\n",
       "      <td>The Middle River Mersey catchment includes Mic...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.43479</td>\n",
       "      <td>-2.31497</td>\n",
       "      <td>River Mersey</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>MULTIPOLYGON (((-2.10956 53.32586, -2.10994 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Manchester</td>\n",
       "      <td>Areas at risk include land and properties arou...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.51869</td>\n",
       "      <td>-2.22357</td>\n",
       "      <td>River Irk</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>MULTIPOLYGON (((-2.22293 53.51522, -2.22231 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Bolton, Bury, Manchester, Oldham, Rochdale, Sa...</td>\n",
       "      <td>The Lower River Irwell catchment also includes...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.48905</td>\n",
       "      <td>-2.28848</td>\n",
       "      <td>River Irwell</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>MULTIPOLYGON (((-2.23183 53.46984, -2.23233 53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Manchester, Stockport</td>\n",
       "      <td>Areas in the locality of Mauldeth Road, includ...</td>\n",
       "      <td>Gtr Mancs Mersey and Ches</td>\n",
       "      <td>53.43947</td>\n",
       "      <td>-2.23322</td>\n",
       "      <td>Cringle Brook</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>MULTIPOLYGON (((-2.22034 53.43457, -2.22038 53...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               county  \\\n",
       "28                                         Manchester   \n",
       "29  Cheshire East, Manchester, Salford, Stockport,...   \n",
       "51                                         Manchester   \n",
       "52  Bolton, Bury, Manchester, Oldham, Rochdale, Sa...   \n",
       "64                              Manchester, Stockport   \n",
       "\n",
       "                                     text_description  \\\n",
       "28  Land adjacent to the River Mersey at West Dids...   \n",
       "29  The Middle River Mersey catchment includes Mic...   \n",
       "51  Areas at risk include land and properties arou...   \n",
       "52  The Lower River Irwell catchment also includes...   \n",
       "64  Areas in the locality of Mauldeth Road, includ...   \n",
       "\n",
       "                    area_name       lat      lon   water_source  \\\n",
       "28  Gtr Mancs Mersey and Ches  53.41100 -2.24160   River Mersey   \n",
       "29  Gtr Mancs Mersey and Ches  53.43479 -2.31497   River Mersey   \n",
       "51  Gtr Mancs Mersey and Ches  53.51869 -2.22357      River Irk   \n",
       "52  Gtr Mancs Mersey and Ches  53.48905 -2.28848   River Irwell   \n",
       "64  Gtr Mancs Mersey and Ches  53.43947 -2.23322  Cringle Brook   \n",
       "\n",
       "                                              polygon  \\\n",
       "28  http://environment.data.gov.uk/flood-monitorin...   \n",
       "29  http://environment.data.gov.uk/flood-monitorin...   \n",
       "51  http://environment.data.gov.uk/flood-monitorin...   \n",
       "52  http://environment.data.gov.uk/flood-monitorin...   \n",
       "64  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                                             geometry  \n",
       "28  POLYGON ((-2.24472 53.4153, -2.24508 53.41532,...  \n",
       "29  MULTIPOLYGON (((-2.10956 53.32586, -2.10994 53...  \n",
       "51  MULTIPOLYGON (((-2.22293 53.51522, -2.22231 53...  \n",
       "52  MULTIPOLYGON (((-2.23183 53.46984, -2.23233 53...  \n",
       "64  MULTIPOLYGON (((-2.22034 53.43457, -2.22038 53...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrangle the flood data and prepare it for analysis\n",
    "flood_data_gdf = wrangle_flood_data()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "flood_data_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prices_paid_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 51\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_df\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     50\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m merge_dataframes(\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mprices_paid_df\u001b[49m, employment_df, hpi_df, income_df, crime_df, flood_gdf)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the merged dataframe\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_data\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prices_paid_df' is not defined"
     ]
    }
   ],
   "source": [
    "def merge_dataframes(prices_paid_df, employment_df, hpi_df, income_df, crime_df, flood_gdf):\n",
    "    \"\"\"\n",
    "    Merge multiple dataframes and a geodataframe on relevant keys and spatial relationships.\n",
    "\n",
    "    Parameters:\n",
    "    prices_paid_df (DataFrame): Cleaned prices paid data.\n",
    "    employment_df (DataFrame): Employment data.\n",
    "    hpi_df (DataFrame): House price index (HPI) data.\n",
    "    income_df (DataFrame): Income data.\n",
    "    crime_df (DataFrame): Crime data.\n",
    "    flood_gdf (GeoDataFrame): Flood risk area data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A merged dataframe combining all relevant data.\n",
    "    \"\"\"\n",
    "    # Merge employment data based on town and year\n",
    "    prices_paid_df['year'] = prices_paid_df['transfer_date'].dt.year\n",
    "    merged_df = pd.merge(prices_paid_df, employment_df, left_on=[\n",
    "                        'town', 'year'], right_on=['area_name', 'year'], how='left')\n",
    "\n",
    "    # Merge HPI data based on area_code and date (rounding date to nearest month)\n",
    "    merged_df = pd.merge(merged_df, hpi_df, left_on=[\n",
    "                         'district', 'transfer_date'], right_on=['region_name', 'date'], how='left')\n",
    "\n",
    "    # Merge income data based on area_code and year\n",
    "    merged_df = pd.merge(merged_df, income_df, left_on=[\n",
    "                        'district', 'year'], right_on=['area_name', 'year'], how='left')\n",
    "\n",
    "    # Merge crime data based on nearest location (latitude and longitude)\n",
    "    # For this, we'll need to create spatial points from the latitude and longitude\n",
    "    prices_paid_gdf = gpd.GeoDataFrame(merged_df, geometry=gpd.points_from_xy(\n",
    "        merged_df.longitude, merged_df.latitude))\n",
    "    crime_gdf = gpd.GeoDataFrame(crime_df, geometry=gpd.points_from_xy(\n",
    "        crime_df.longitude, crime_df.latitude))\n",
    "\n",
    "    # Spatial join to find the nearest crime data points\n",
    "    merged_gdf = gpd.sjoin_nearest(\n",
    "        prices_paid_gdf, crime_gdf, how='left', distance_col=\"crime_distance\")\n",
    "\n",
    "    # Merge flood risk area data based on spatial join\n",
    "    merged_gdf = gpd.sjoin(merged_gdf, flood_gdf, how='left', op='intersects')\n",
    "\n",
    "    # Convert the final GeoDataFrame back to a DataFrame if needed\n",
    "    final_df = pd.DataFrame(merged_gdf.drop(columns='geometry'))\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "merged_data = merge_dataframes(\n",
    "    prices_paid_df, employment_df, hpi_df, income_df, crime_df, flood_gdf)\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_copy = prices_paid_data_df.copy()\n",
    "employment_copy = employment_data_df.copy()\n",
    "income_copy = income_data_df.copy()\n",
    "flood_copy = flood_data_gdf.copy()\n",
    "hpi_copy = hpi_data_df.copy()\n",
    "crime_copy = crime_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge employment data based on area_code and year\n",
    "prices_copy['year'] = prices_copy['transfer_date'].dt.year\n",
    "# Merge using district instead of town\n",
    "merged_prices_employment_copy = pd.merge(\n",
    "    prices_copy,\n",
    "    employment_copy,\n",
    "    left_on=['district', 'year'],\n",
    "    right_on=['area_name', 'year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill any missing values in the merged dataframe\n",
    "merged_prices_employment_copy.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(merged_prices_employment_copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_prices_employment_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpi_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round transfer_date to the nearest month to match HPI data\n",
    "merged_prices_employment_copy['transfer_date_month'] = merged_prices_employment_copy['transfer_date'].dt.to_period(\n",
    "    'M')\n",
    "\n",
    "# Convert HPI date to the same period format for merging\n",
    "hpi_copy['date_month'] = pd.to_datetime(hpi_copy['date']).dt.to_period('M')\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_prices_employment_hpi_copy = pd.merge(\n",
    "    merged_prices_employment_copy,\n",
    "    hpi_copy,\n",
    "    left_on=['district', 'transfer_date_month'],\n",
    "    right_on=['region_name', 'date_month'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill any missing values in the merged dataframe\n",
    "merged_prices_employment_hpi_copy.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Drop the temporary 'transfer_date_month' and 'date_month' columns if needed\n",
    "merged_prices_employment_hpi_copy.drop(\n",
    "    columns=['transfer_date_month', 'date_month'], inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "merged_prices_employment_hpi_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_prices_employment_hpi_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming merged_prices_employment_hpi_copy is the DataFrame after merging prices, employment, and HPI data\n",
    "\n",
    "# Merge the dataframes based on district (area_name) and year\n",
    "merged_prices_employment_hpi_income_copy = pd.merge(\n",
    "    merged_prices_employment_hpi_copy,\n",
    "    income_copy,\n",
    "    left_on=['district', 'year'],\n",
    "    right_on=['area_name', 'year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Forward fill any missing values in the merged dataframe\n",
    "merged_prices_employment_hpi_income_copy.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "merged_prices_employment_hpi_income_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_prices_employment_hpi_income_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
